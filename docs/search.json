[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cookbook to Draw KHCoder-like Visualizations Using R",
    "section": "",
    "text": "Preface\n\nCooking icons created by Freepik - Flaticon\n\n本文についてはCC-BY、掲載されているコードについてはMITライセンスを適用するものとします。\nなんとなく似たような表現をめざしているだけで、KH Coderでおこなわれている処理をRで再実装することをめざすものではありません。また、論文やレポートに掲載するといったことを想定しているわけではないので、わりと躊躇なくHTMLウィジェットを使っています。\nKH Coderの機能や可視化をもとにしているのではない表現については、小見出しに🍳という絵文字を付けています。",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preproc-menu.html",
    "href": "preproc-menu.html",
    "title": "1  前処理メニュー",
    "section": "",
    "text": "1.1 使用するデータセット\nKH Coderのチュートリアル用のデータを使う。tutorial_data_3x.zipの中に含まれているtutorial_jp/kokoro.xlsというxlsファイルを次のように読み込んでおく。\nCode\ntbl &lt;-\n  readxl::read_xls(\"tutorial_jp/kokoro.xls\",\n    col_names = c(\"text\", \"section\", \"chapter\", \"label\"),\n    skip = 1\n  ) |&gt;\n  dplyr::mutate(\n    doc_id = dplyr::row_number(),\n    dplyr::across(where(is.character), ~ audubon::strj_normalize(.))\n  ) |&gt;\n  dplyr::filter(!gibasa::is_blank(text)) |&gt;\n  dplyr::relocate(doc_id, text, section, label, chapter)\n\ntbl\n#&gt; # A tibble: 1,213 × 5\n#&gt;    doc_id text                                            section label  chapter\n#&gt;     &lt;int&gt; &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  \n#&gt;  1      1 私はその人を常に先生と呼んでいた。だからここで… [1]上_… 上・一 1_01   \n#&gt;  2      2 私が先生と知り合いになったのは鎌倉である。その… [1]上_… 上・一 1_01   \n#&gt;  3      3 学校の授業が始まるにはまだ大分日数があるので鎌… [1]上_… 上・一 1_01   \n#&gt;  4      4 宿は鎌倉でも辺鄙な方角にあった。玉突きだのアイ… [1]上_… 上・一 1_01   \n#&gt;  5      5 私は毎日海へはいりに出掛けた。古い燻ぶり返った… [1]上_… 上・一 1_01   \n#&gt;  6      6 私は実に先生をこの雑沓の間に見付け出したのであ… [1]上_… 上・一 1_01   \n#&gt;  7      7 私がその掛茶屋で先生を見た時は、先生がちょうど… [1]上_… 上・二 1_02   \n#&gt;  8      8 その西洋人の優れて白い皮膚の色が、掛茶屋へ入る… [1]上_… 上・二 1_02   \n#&gt;  9      9 彼はやがて自分の傍を顧みて、そこにこごんでいる… [1]上_… 上・二 1_02   \n#&gt; 10     10 私は単に好奇心のために、並んで浜辺を下りて行く… [1]上_… 上・二 1_02   \n#&gt; # ℹ 1,203 more rows\nこのデータでは、夏目漱石の『こころ』が段落（doc_id）ごとにひとつのテキストとして打ち込まれている。『こころ』は上中下の3部（section）で構成されていて、それぞれの部が複数の章（label, chapter）に分かれている。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#語の抽出a.2.2",
    "href": "preproc-menu.html#語の抽出a.2.2",
    "title": "1  前処理メニュー",
    "section": "1.2 語の抽出（A.2.2）",
    "text": "1.2 語の抽出（A.2.2）\ngibasaを使って形態素解析をおこない、語を抽出する。\nこのデータをIPA辞書を使って形態素解析すると、延べ語数は105,000語程度になる。これくらいの語数であれば、形態素解析した結果をデータフレームとしてメモリ上に読み込んでも問題ないと思われるが、ここではより大規模なテキストデータを扱う場合を想定し、結果をDuckDBデータベースに書き込むことにする。\nここではchapterごとにグルーピングしながら、段落は文に分割せずに処理している。MeCabはバッファサイズの都合上、一度に262万字くらいまで一つの文として入力できるらしいが、極端に長い入力に対してはコスト計算ができず、エラーが出る可能性がある。また、多くの文を与えればそれだけ多くの行からなるデータフレームが返されるため、一度に処理する分量は利用している環境にあわせて適当に加減したほうがよい。\nKH Coderでは、IPA辞書の品詞体系をもとに変更した品詞体系が使われている。そのため、KH Coderで前処理した結果をある程度再現するためには、一部の品詞情報を書き換える必要がある。KH Coder内で使われている品詞体系については、KH Coderのレファレンスを参照されたい。\nまた、このデータを使っているチュートリアルでは、強制抽出する語として「一人」「二人」という語を指定している。こうした語についてはMeCabのユーザー辞書に追加してしまったほうがよいが、簡単に処理するために、ここではgibasaの制約付き解析機能によって「タグ」として抽出している（KH Coderは強制抽出した語に対して「タグ」という品詞名を与える）。\n\n\nCode\nsuppressPackageStartupMessages({\n  library(duckdb)\n})\ndrv &lt;- duckdb::duckdb()\n\nif (!fs::file_exists(\"tutorial_jp/kokoro.duckdb\")) {\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = FALSE)\n\n  dbCreateTable(\n    con, \"tokens\",\n    data.frame(\n      doc_id = integer(),\n      section = character(),\n      label = character(),\n      token_id = integer(),\n      token = character(),\n      pos = character(),\n      original = character(),\n      stringsAsFactors = FALSE\n    )\n  )\n\n  tbl |&gt;\n    dplyr::group_by(chapter) |&gt;\n    dplyr::group_walk(~ {\n      df &lt;- .x |&gt;\n        dplyr::mutate(\n          text = stringi::stri_replace_all_regex(text, \"(?&lt;codes&gt;([一二三四五六七八九]{1}人))\", \"\\n${codes}\\tタグ\\n\") |&gt;\n            stringi::stri_trim_both()\n        ) |&gt;\n        gibasa::tokenize(text, doc_id, partial = TRUE) |&gt;\n        gibasa::prettify(\n          col_select = c(\"POS1\", \"POS2\", \"POS3\", \"Original\")\n        ) |&gt;\n        dplyr::mutate(\n          pos = dplyr::case_when(\n            (POS1 == \"タグ\") ~ \"タグ\",\n            (is.na(Original) & stringr::str_detect(token, \"^[[:alpha:]]+$\")) ~ \"未知語\",\n            (POS1 == \"感動詞\") ~ \"感動詞\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Han}]{1}$\")) ~ \"名詞C\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"名詞B\",\n            (POS1 == \"名詞\" & POS2 == \"一般\") ~ \"名詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"地域\") ~ \"地名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"人名\") ~ \"人名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"組織\") ~ \"組織名\",\n            (POS1 == \"名詞\" & POS2 == \"形容動詞語幹\") ~ \"形容動詞\",\n            (POS1 == \"名詞\" & POS2 == \"ナイ形容詞語幹\") ~ \"ナイ形容詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\") ~ \"固有名詞\",\n            (POS1 == \"名詞\" & POS2 == \"サ変接続\") ~ \"サ変名詞\",\n            (POS1 == \"名詞\" & POS2 == \"副詞可能\") ~ \"副詞可能\",\n            (POS1 == \"動詞\" & POS2 == \"自立\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"動詞B\",\n            (POS1 == \"動詞\" & POS2 == \"自立\") ~ \"動詞\",\n            (POS1 == \"形容詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"形容詞B\",\n            (POS1 == \"形容詞\" & POS2 == \"非自立\") ~ \"形容詞（非自立）\",\n            (POS1 == \"形容詞\") ~ \"形容詞\",\n            (POS1 == \"副詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"副詞B\",\n            (POS1 == \"副詞\") ~ \"副詞\",\n            (POS1 == \"助動詞\" & Original %in% c(\"ない\", \"まい\", \"ぬ\", \"ん\")) ~ \"否定助動詞\",\n            .default = \"その他\"\n          )\n        ) |&gt;\n        dplyr::select(doc_id, section, label, token_id, token, pos, Original) |&gt;\n        dplyr::rename(original = Original)\n\n      dbAppendTable(con, \"tokens\", df)\n    })\n} else {\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = TRUE)\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#コーディングルールa.2.5",
    "href": "preproc-menu.html#コーディングルールa.2.5",
    "title": "1  前処理メニュー",
    "section": "1.3 コーディングルール（A.2.5）",
    "text": "1.3 コーディングルール（A.2.5）\nKH Coderの強力な機能のひとつとして、「コーディングルール」によるトークンへのタグ付けというのがある。KH Coderのコーディングルールはかなり複雑な記法を扱うため、Rで完璧に再現するには相応の手間がかかる。一方で、コードを与えるべき抽出語を基本形とマッチングする程度であれば、次のように比較的少ないコード量で似たようなことを実現できる。\n\n\nCode\nrules &lt;- list(\n  \"人の死\" = c(\"死後\", \"死病\", \"死期\", \"死因\", \"死骸\", \"生死\", \"自殺\", \"殉死\", \"頓死\", \"変死\", \"亡\", \"死ぬ\", \"亡くなる\", \"殺す\", \"亡くす\", \"死\"),\n  \"恋愛\" = c(\"愛\", \"恋\", \"愛す\", \"愛情\", \"恋人\", \"愛人\", \"恋愛\", \"失恋\", \"恋しい\"),\n  \"友情\" = c(\"友達\", \"友人\", \"旧友\", \"親友\", \"朋友\", \"友\", \"級友\"),\n  \"信用・不信\" = c(\"信用\", \"信じる\", \"信ずる\", \"不信\", \"疑い\", \"疑惑\", \"疑念\", \"猜疑\", \"狐疑\", \"疑問\", \"疑い深い\", \"疑う\", \"疑る\", \"警戒\"),\n  \"病気\" = c(\"医者\", \"病人\", \"病室\", \"病院\", \"病症\", \"病状\", \"持病\", \"死病\", \"主治医\", \"精神病\", \"仮病\", \"病気\", \"看病\", \"大病\", \"病む\", \"病\")\n)\n\ncodes &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(original %in% purrr::flatten_chr(rules)) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::mutate(\n    codings = purrr::map(\n      original,\n      ~ purrr::imap(rules, \\(.x, .y) tibble::tibble(code = .y, flag = . %in% .x)) |&gt;\n        purrr::list_rbind() |&gt;\n        dplyr::filter(flag == TRUE) |&gt;\n        dplyr::select(!flag)\n    )\n  ) |&gt;\n  tidyr::unnest(codings)\n\ncodes\n#&gt; # A tibble: 537 × 8\n#&gt;    doc_id section        label  token_id token    pos      original code      \n#&gt;     &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     \n#&gt;  1      2 [1]上_先生と私 上・一       36 友達     名詞     友達     友情      \n#&gt;  2      2 [1]上_先生と私 上・一       96 友達     名詞     友達     友情      \n#&gt;  3      2 [1]上_先生と私 上・一      115 病気     サ変名詞 病気     病気      \n#&gt;  4      2 [1]上_先生と私 上・一      124 友達     名詞     友達     友情      \n#&gt;  5      2 [1]上_先生と私 上・一      128 信じ     動詞     信じる   信用・不信\n#&gt;  6      2 [1]上_先生と私 上・一      132 友達     名詞     友達     友情      \n#&gt;  7      2 [1]上_先生と私 上・一      240 病気     サ変名詞 病気     病気      \n#&gt;  8      3 [1]上_先生と私 上・一       44 友達     名詞     友達     友情      \n#&gt;  9     19 [1]上_先生と私 上・三      207 疑っ     動詞     疑う     信用・不信\n#&gt; 10     21 [1]上_先生と私 上・四      161 亡くなっ 動詞     亡くなる 人の死    \n#&gt; # ℹ 527 more rows",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#抽出語リストa.3.4",
    "href": "preproc-menu.html#抽出語リストa.3.4",
    "title": "1  前処理メニュー",
    "section": "1.4 抽出語リスト（A.3.4）",
    "text": "1.4 抽出語リスト（A.3.4）\n「エクスポート」メニューから得られるような抽出語リストをデータフレームとして得る例。\nExcel向けの出力は見やすいようにカラムを分けているが、Rのデータフレームとして扱うならtidyな縦長のデータにしたほうがよい。\n\n1.4.1 品詞別・上位15語\n\n\nCode\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 15, by = pos) |&gt;\n  dplyr::collect()\n#&gt; # A tibble: 232 × 3\n#&gt;    token  pos          n\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 前     副詞可能   169\n#&gt;  2 今     副詞可能   140\n#&gt;  3 場合   副詞可能    38\n#&gt;  4 過去   副詞可能    34\n#&gt;  5 結果   副詞可能    30\n#&gt;  6 すべて 副詞可能    28\n#&gt;  7 平生   副詞可能    26\n#&gt;  8 時間   副詞可能    25\n#&gt;  9 今度   副詞可能    24\n#&gt; 10 始め   副詞可能    22\n#&gt; # ℹ 222 more rows\n\n\n\n\n1.4.2 頻出150語\n\n\nCode\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 150) |&gt;\n  dplyr::collect()\n#&gt; # A tibble: 152 × 3\n#&gt;    token  pos        n\n#&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;\n#&gt;  1 先生   名詞     595\n#&gt;  2 K      未知語   411\n#&gt;  3 奥さん 名詞     388\n#&gt;  4 思う   動詞     293\n#&gt;  5 父     名詞C    269\n#&gt;  6 自分   名詞     264\n#&gt;  7 見る   動詞     225\n#&gt;  8 聞く   動詞     218\n#&gt;  9 出る   動詞     179\n#&gt; 10 人     名詞C    176\n#&gt; # ℹ 142 more rows",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#文書抽出語表a.3.5",
    "href": "preproc-menu.html#文書抽出語表a.3.5",
    "title": "1  前処理メニュー",
    "section": "1.5 「文書・抽出語」表（A.3.5）",
    "text": "1.5 「文書・抽出語」表（A.3.5）\nいわゆる文書単語行列の例。dplyr::collectした後にtidyr::pivot_wider()などで横に展開してもよいが、多くの場合、疎行列のオブジェクトにしてしまったほうが、この後にRでの解析に用いる上では扱いやすいと思われる。quantedaのdfmオブジェクトをふつうの密な行列にしたいときは、as.matrix(dfm)すればよい。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(min_termfreq = 75, termfreq_type = \"rank\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n#&gt; Document-feature matrix of: 1,189 documents, 76 features (92.93% sparse) and 1 docvar.\n#&gt;     features\n#&gt; docs 書く/動詞 心持/名詞 来る/動詞 人/名詞C 持つ/動詞 眼/名詞C 頭/名詞C\n#&gt;    1         1         1         0        2         0        0        0\n#&gt;    2         0         0         2        0         0        0        0\n#&gt;    3         0         0         0        0         0        0        0\n#&gt;    4         0         0         0        0         0        0        0\n#&gt;    5         0         0         1        1         0        0        1\n#&gt;    6         0         0         0        1         1        0        0\n#&gt;     features\n#&gt; docs 見る/動詞 見える/動詞 付く/動詞\n#&gt;    1         0           0         0\n#&gt;    2         0           0         0\n#&gt;    3         0           0         0\n#&gt;    4         0           0         0\n#&gt;    5         0           0         0\n#&gt;    6         0           0         0\n#&gt; [ reached max_ndoc ... 1,183 more documents, reached max_nfeat ... 66 more features ]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#文書コード表a.3.6",
    "href": "preproc-menu.html#文書コード表a.3.6",
    "title": "1  前処理メニュー",
    "section": "1.6 「文書・コード」表（A.3.6）",
    "text": "1.6 「文書・コード」表（A.3.6）\n「文書・コード」行列の例。コードの出現頻度ではなく「コードの有無をあらわす2値変数」を出力する。\n\n\nCode\ndfm &lt;- codes |&gt;\n  dplyr::count(doc_id, code) |&gt;\n  tidytext::cast_dfm(doc_id, code, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n#&gt; Document-feature matrix of: 294 documents, 5 features (76.19% sparse) and 1 docvar.\n#&gt;     features\n#&gt; docs 信用・不信 友情 病気 人の死 恋愛\n#&gt;   2           1    1    1      0    0\n#&gt;   3           0    1    0      0    0\n#&gt;   19          1    0    0      0    0\n#&gt;   21          0    0    0      1    0\n#&gt;   37          0    0    0      1    0\n#&gt;   49          0    1    0      0    0\n#&gt; [ reached max_ndoc ... 288 more documents ]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "preproc-menu.html#抽出語文脈ベクトル表a.3.7",
    "href": "preproc-menu.html#抽出語文脈ベクトル表a.3.7",
    "title": "1  前処理メニュー",
    "section": "1.7 「抽出語・文脈ベクトル」表（A.3.7）",
    "text": "1.7 「抽出語・文脈ベクトル」表（A.3.7）\n\n1.7.1 word2vec🍳\n\n\n\n\n\n\nCaution\n\n\n\n以下で扱っているベクトルは、KH Coderにおける「抽出語・文脈ベクトル」とは異なるものです\n\n\nKH Coderにおける「文脈ベクトル」は、これを使って抽出語メニューからおこなえるような分析をすることによって、「似たような使われ方をする語を調べる」使い方をするためのものだと思われる。\n「似たような使われ方をする語を調べる」ためであれば、単語埋め込みを使ってもよさそう。ただし、KH Coderの「文脈ベクトル」を使う場合の「似たような使われ方をする」が、あくまで分析対象とする文書のなかでという意味であるのに対して、単語埋め込みを使う場合では、埋め込みの学習に使われた文書のなかでという意味になってしまう点には注意。\n試しに、既存のword2vecモデルから単語ベクトルを読み込んでみる。ここでは、Wikipedia2Vecの100次元のものを使う。だいぶ古いモデルだが、MeCab+IPA辞書で分かち書きされた語彙を使っていることが確認できる単語埋め込みとなると（参考）、これくらいの時期につくられたものになりそう。\n\n\nCode\n# word2vecのテキスト形式のファイルは、先頭行に埋め込みの「行数 列数」が書かれている\nreadr::read_lines(\"tutorial_jp/jawiki_20180420_100d.txt.bz2\", n_max = 1)\n#&gt; [1] \"1593143 100\"\n\n# 下のほうは低頻度語で、全部読み込む必要はないと思われるので、ここでは先頭から1e5行だけ読み込む\nembeddings &lt;-\n  readr::read_delim(\n    \"tutorial_jp/jawiki_20180420_100d.txt.bz2\",\n    delim = \" \",\n    col_names = c(\"token\", paste0(\"dim_\", seq_len(100))),\n    skip = 1,\n    n_max = 1e5,\n    show_col_types = FALSE\n  )\n\n# メモリ上でのサイズ\nlobstr::obj_size(embeddings)\n#&gt; 117.94 MB\n\n\nこのうち、分析対象の文書に含まれる語彙のベクトルだけを適当に取り出しておく。\n\n\nCode\nembeddings &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::transmute(\n    doc_id = doc_id,\n    token = token,\n    label = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::distinct(doc_id, token, .keep_all = TRUE) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::inner_join(embeddings, by = dplyr::join_by(token == token))\n\nembeddings\n#&gt; # A tibble: 10,578 × 103\n#&gt;    doc_id token  label        dim_1  dim_2   dim_3   dim_4  dim_5   dim_6  dim_7\n#&gt;     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1      1 世間   世間/名詞   0.367  -0.711 -0.418  -0.261  -0.296 -0.0888 -0.216\n#&gt;  2      1 憚     憚/未知語   0.163  -1.03  -0.573   0.104   0.059 -0.319   0.235\n#&gt;  3      2 夏休み 夏休み/名…  0.204   0.444 -0.422  -0.0065 -0.472  0.0469  0.312\n#&gt;  4      2 東京   東京/地名   0.539  -0.347  0.0582  0.190  -0.399 -0.554  -0.414\n#&gt;  5      6 清め   清め/動詞   0.456  -0.755 -0.436   0.396   0.611 -0.878   1.05 \n#&gt;  6      6 間     間/名詞C    0.492   0.129  0.0783  0.0069 -0.255 -0.151  -0.275\n#&gt;  7      7 間     間/名詞C    0.492   0.129  0.0783  0.0069 -0.255 -0.151  -0.275\n#&gt;  8      8 浜     浜/名詞C    0.809  -0.270  0.212   0.557  -0.908 -0.274  -0.214\n#&gt;  9      8 肉     肉/名詞C    0.667  -0.113 -0.0736  0.328  -0.134 -0.0013 -0.342\n#&gt; 10     10 多人数 多人数/名… -0.0069 -0.123 -0.2     0.481   0.198 -0.0509  0.105\n#&gt; # ℹ 10,568 more rows\n#&gt; # ℹ 93 more variables: dim_8 &lt;dbl&gt;, dim_9 &lt;dbl&gt;, dim_10 &lt;dbl&gt;, dim_11 &lt;dbl&gt;,\n#&gt; #   dim_12 &lt;dbl&gt;, dim_13 &lt;dbl&gt;, dim_14 &lt;dbl&gt;, dim_15 &lt;dbl&gt;, dim_16 &lt;dbl&gt;,\n#&gt; #   dim_17 &lt;dbl&gt;, dim_18 &lt;dbl&gt;, dim_19 &lt;dbl&gt;, dim_20 &lt;dbl&gt;, dim_21 &lt;dbl&gt;,\n#&gt; #   dim_22 &lt;dbl&gt;, dim_23 &lt;dbl&gt;, dim_24 &lt;dbl&gt;, dim_25 &lt;dbl&gt;, dim_26 &lt;dbl&gt;,\n#&gt; #   dim_27 &lt;dbl&gt;, dim_28 &lt;dbl&gt;, dim_29 &lt;dbl&gt;, dim_30 &lt;dbl&gt;, dim_31 &lt;dbl&gt;,\n#&gt; #   dim_32 &lt;dbl&gt;, dim_33 &lt;dbl&gt;, dim_34 &lt;dbl&gt;, dim_35 &lt;dbl&gt;, dim_36 &lt;dbl&gt;, …\n\n\n\n\n1.7.2 独立成分分析（ICA）🍳\nword2vecを含む埋め込み表現は、「独立成分分析（ICA）で次元削減することで、人間にとって解釈性の高い成分を取り出すことができる」ことが知られている（参考）。これを応用すると、ICAで取り出した成分をもとにして、コーディングルールにするとよさそうなカテゴリや語彙を探索できるかもしれない。\n\n\nCode\nica &lt;- embeddings |&gt;\n  dplyr::select(label, dplyr::starts_with(\"dim\")) |&gt;\n  dplyr::distinct() |&gt;\n  tibble::column_to_rownames(\"label\") |&gt;\n  as.matrix() |&gt;\n  fastICA::fastICA(n.comp = 20)\n\ndat &lt;- ica$S |&gt;\n  rlang::as_function(~ {\n    . * sign(moments::skewness(.)) # 正方向のスコアだけを扱うため、歪度が負の成分を変換する\n  })() |&gt;\n  dplyr::as_tibble(\n    rownames = NA,\n    .name_repair = ~ paste(\"dim\", stringr::str_pad(seq_along(.x), width = 2, pad = \"0\"), sep = \"_\")\n  ) |&gt;\n  tibble::rownames_to_column(\"label\") |&gt;\n  tidyr::pivot_longer(cols = !label, names_to = \"dim\", values_to = \"score\")\n\n\nここでは正方向のスコアだけを扱うため、歪度が負の成分を正方向に変換する処理をしている。もちろん、実際には負の方向のスコアが大きい語彙とあわせて解釈したほうがわかりやすい成分もありそうなことからも、そのあたりも含めてそれぞれの成分を探索し、ほかの分析とも組み合わせながらコーディングルールを考えるべきだろう。\n各成分の正方向のスコアが大きい語彙を図示すると次のような感じになる。\n\n\nCode\nlibrary(ggplot2)\n\ndat |&gt;\n  dplyr::slice_max(score, n = 8, by = dim) |&gt;\n  ggplot(aes(x = reorder(label, score), y = score)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(vars(dim), ncol = 4, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\n実際にはこうして得られたスコアの大きい語をそのままコーディングルールとして採用することはしないほうがよいが、ここから次のようにして「文書・コード」表をつくることもできる。\n\n\nCode\nrules &lt;- dat |&gt;\n  dplyr::slice_max(score, n = 10, by = dim) |&gt;\n  dplyr::reframe(\n    label = list(label),\n    .by = dim\n  ) |&gt;\n  tibble::deframe()\n\ncodes &lt;-\n  embeddings |&gt;\n  dplyr::select(doc_id, label) |&gt;\n  dplyr::filter(label %in% purrr::flatten_chr(rules)) |&gt;\n  dplyr::mutate(\n    codings = purrr::map(\n      label,\n      ~ purrr::imap(rules, \\(.x, .y) tibble::tibble(code = .y, flag = . %in% .x)) |&gt;\n        purrr::list_rbind() |&gt;\n        dplyr::filter(flag == TRUE) |&gt;\n        dplyr::select(!flag)\n    )\n  ) |&gt;\n  tidyr::unnest(codings)\n\ndfm &lt;- codes |&gt;\n  dplyr::count(doc_id, code) |&gt;\n  tidytext::cast_dfm(doc_id, code, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\ndfm\n#&gt; Document-feature matrix of: 373 documents, 20 features (93.15% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs dim_02 dim_20 dim_10 dim_15 dim_11 dim_05 dim_14 dim_17 dim_06 dim_08\n#&gt;   2       1      0      0      0      0      0      0      0      0      0\n#&gt;   4       1      1      0      0      0      0      0      0      0      0\n#&gt;   6       1      0      1      0      0      0      0      0      0      0\n#&gt;   8       0      0      0      1      0      0      0      0      0      0\n#&gt;   9       0      0      0      0      1      0      0      0      0      0\n#&gt;   10      1      0      0      0      0      0      0      0      0      0\n#&gt; [ reached max_ndoc ... 367 more documents, reached max_nfeat ... 10 more features ]\n\n\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version    date (UTC) lib source\n#&gt;  audubon        0.5.1.9001 2023-09-03 [1] https://paithiov909.r-universe.dev (R 4.3.1)\n#&gt;  bit            4.0.5      2022-11-15 [1] CRAN (R 4.3.0)\n#&gt;  bit64          4.0.5      2020-08-30 [1] CRAN (R 4.3.0)\n#&gt;  blob           1.2.4      2023-03-17 [1] RSPM (R 4.3.0)\n#&gt;  cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  cellranger     1.1.0      2016-07-27 [1] RSPM (R 4.3.0)\n#&gt;  cli            3.6.2      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  colorspace     2.1-0      2023-01-23 [1] RSPM (R 4.3.0)\n#&gt;  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#&gt;  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.3)\n#&gt;  DBI          * 1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  dbplyr         2.4.0      2023-10-26 [1] RSPM (R 4.3.0)\n#&gt;  digest         0.6.34     2024-01-11 [1] CRAN (R 4.3.2)\n#&gt;  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  duckdb       * 0.9.2-1    2023-11-28 [1] RSPM (R 4.3.2)\n#&gt;  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.1)\n#&gt;  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n#&gt;  farver         2.1.1      2022-07-06 [1] RSPM (R 4.3.0)\n#&gt;  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fastmatch      1.1-4      2023-08-18 [1] RSPM (R 4.3.0)\n#&gt;  fs             1.6.3      2023-07-20 [1] RSPM (R 4.3.1)\n#&gt;  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2      * 3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  gibasa         1.1.0      2024-02-17 [1] https://paithiov909.r-universe.dev (R 4.3.2)\n#&gt;  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n#&gt;  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.1)\n#&gt;  hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;  htmltools      0.5.7      2023-11-03 [1] RSPM (R 4.3.0)\n#&gt;  htmlwidgets    1.6.4      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  httpgd         1.3.1      2023-01-30 [1] CRAN (R 4.3.0)\n#&gt;  janeaustenr    1.0.0      2022-08-26 [1] RSPM (R 4.3.0)\n#&gt;  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n#&gt;  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.1)\n#&gt;  labeling       0.4.3      2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  later          1.3.2      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  lattice        0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  lifecycle      1.0.4      2023-11-07 [1] RSPM (R 4.3.0)\n#&gt;  lobstr         1.1.2      2022-06-22 [1] RSPM (R 4.3.0)\n#&gt;  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  Matrix         1.6-3      2023-11-14 [4] CRAN (R 4.3.2)\n#&gt;  memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  munsell        0.5.0      2018-06-12 [1] RSPM (R 4.3.0)\n#&gt;  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  prettyunits    1.2.0      2023-09-24 [1] RSPM (R 4.3.0)\n#&gt;  purrr          1.0.2      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  quanteda       3.3.1      2023-05-18 [1] RSPM (R 4.3.0)\n#&gt;  R.cache        0.16.0     2022-07-21 [1] CRAN (R 4.3.0)\n#&gt;  R.methodsS3    1.8.2      2022-06-13 [1] CRAN (R 4.3.0)\n#&gt;  R.oo           1.26.0     2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  R.utils        2.12.3     2023-11-18 [1] CRAN (R 4.3.2)\n#&gt;  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp           1.0.12     2024-01-09 [1] RSPM (R 4.3.0)\n#&gt;  RcppParallel   5.1.7      2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;  readr          2.1.5      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  readxl         1.4.3      2023-07-06 [1] RSPM (R 4.3.0)\n#&gt;  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  rmarkdown      2.25       2023-09-18 [1] RSPM (R 4.3.0)\n#&gt;  scales         1.3.0      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  sessioninfo    1.2.2      2021-12-06 [1] RSPM (R 4.3.0)\n#&gt;  SnowballC      0.7.1      2023-04-25 [1] RSPM (R 4.3.0)\n#&gt;  stopwords      2.3        2021-10-28 [1] RSPM (R 4.3.0)\n#&gt;  stringi        1.8.3      2024-01-28 [1] https://gagolews.r-universe.dev (R 4.3.2)\n#&gt;  styler         1.10.2     2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  systemfonts    1.0.5      2023-10-09 [1] RSPM (R 4.3.0)\n#&gt;  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyr          1.3.1      2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidytext       0.4.1      2023-01-07 [1] RSPM (R 4.3.0)\n#&gt;  tokenizers     0.3.0      2022-12-22 [1] RSPM (R 4.3.0)\n#&gt;  tzdb           0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n#&gt;  utf8           1.2.4      2023-10-22 [1] RSPM (R 4.3.0)\n#&gt;  V8             4.4.2      2024-02-15 [1] CRAN (R 4.3.2)\n#&gt;  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#&gt;  vroom          1.6.5      2023-12-05 [1] RSPM (R 4.3.0)\n#&gt;  withr          3.0.0      2024-01-16 [1] RSPM (R 4.3.0)\n#&gt;  xfun           0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  yaml           2.3.8      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.3\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "words-menu-1.html",
    "href": "words-menu-1.html",
    "title": "2  抽出語メニュー1",
    "section": "",
    "text": "2.1 抽出語リスト（A.5.1）\n活用語をクリックするとそれぞれの活用の出現頻度も見れるUIについては、Rだとどうすれば実現できるかわからない。\nCode\ndat &lt;- dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::filter(n &gt;= 100) |&gt;\n  dplyr::collect()\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat)\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "words-menu-1.html#出現回数の分布a.5.2",
    "href": "words-menu-1.html#出現回数の分布a.5.2",
    "title": "2  抽出語メニュー1",
    "section": "2.2 出現回数の分布（A.5.2）",
    "text": "2.2 出現回数の分布（A.5.2）\n\n2.2.1 度数分布表\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::summarise(\n    degree = sum(n, na.rm = TRUE),\n    .by = n\n  ) |&gt;\n  dplyr::mutate(\n    prop = degree / sum(degree, na.rm = TRUE)\n  ) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::compute() |&gt;\n  dplyr::mutate(\n    cum_degree = cumsum(degree),\n    cum_prop = cumsum(prop)\n  ) |&gt;\n  dplyr::collect()\n\ndat\n#&gt; # A tibble: 98 × 5\n#&gt;        n degree   prop cum_degree cum_prop\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1   2938 0.111        2938    0.111\n#&gt;  2     2   1934 0.0729       4872    0.184\n#&gt;  3     3   1506 0.0568       6378    0.240\n#&gt;  4     4   1248 0.0470       7626    0.287\n#&gt;  5     5    955 0.0360       8581    0.323\n#&gt;  6     6    768 0.0289       9349    0.352\n#&gt;  7     7    868 0.0327      10217    0.385\n#&gt;  8     8    592 0.0223      10809    0.407\n#&gt;  9     9    531 0.0200      11340    0.427\n#&gt; 10    10    570 0.0215      11910    0.449\n#&gt; # ℹ 88 more rows\n\n\n\n\n2.2.2 折れ線グラフ\n\n\nCode\ndat |&gt;\n  dplyr::filter(cum_prop &lt; .8) |&gt;\n  ggplot(aes(x = n, y = degree)) +\n  geom_line() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"度数\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "words-menu-1.html#文書数の分布a.5.3",
    "href": "words-menu-1.html#文書数の分布a.5.3",
    "title": "2  抽出語メニュー1",
    "section": "2.3 文書数の分布（A.5.3）",
    "text": "2.3 文書数の分布（A.5.3）\n\n2.3.1 ヒストグラム🍳\n段落（doc_id）ではなく、章ラベル（label）でグルーピングして集計している。docfreqについて上でやったのと同様に処理すれば度数分布表にできる。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = paste(token, pos, sep = \"/\")) |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(label, token, n) |&gt;\n  quanteda.textstats::textstat_frequency() |&gt;\n  dplyr::as_tibble()\n\ndat\n#&gt; # A tibble: 7,140 × 5\n#&gt;    feature   frequency  rank docfreq group\n#&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt;  1 の/その他      5801     1     110 all  \n#&gt;  2 た/その他      5396     2     110 all  \n#&gt;  3 。/その他      4648     3     110 all  \n#&gt;  4 は/その他      4148     4     110 all  \n#&gt;  5 に/その他      4104     5     110 all  \n#&gt;  6 、/その他      3646     6     110 all  \n#&gt;  7 て/その他      3403     7     110 all  \n#&gt;  8 を/その他      3216     8     110 all  \n#&gt;  9 私/その他      2694     9     110 all  \n#&gt; 10 が/その他      2194    10     110 all  \n#&gt; # ℹ 7,130 more rows\n\n\n度数分布をグラフで確認したいだけなら、このかたちからヒストグラムを描いたほうが楽。\n\n\nCode\ndat |&gt;\n  ggplot(aes(x = docfreq)) +\n  geom_histogram(binwidth = 3) +\n  scale_y_sqrt() +\n  theme_bw() +\n  labs(x = \"文書数\", y = \"度数\")\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Zipf’s law🍳\nよく見かけるグラフ。言語処理100本ノック 2020の39はこれにあたる。\n\n\nCode\ndat |&gt;\n  ggplot(aes(x = rank, y = frequency)) +\n  geom_line() +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE) +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "words-menu-1.html#出現回数文書数のプロットa.5.4",
    "href": "words-menu-1.html#出現回数文書数のプロットa.5.4",
    "title": "2  抽出語メニュー1",
    "section": "2.4 出現回数・文書数のプロット（A.5.4）",
    "text": "2.4 出現回数・文書数のプロット（A.5.4）\n図 A.25のようなグラフの例。ggplot2でgraphics::identifyのようなことをするやり方がわからないので、適当に条件を指定してgghighlightでハイライトしている。\n\n\nCode\ndat |&gt;\n  ggplot(aes(x = frequency, y = docfreq)) +\n  geom_jitter() +\n  gghighlight::gghighlight(\n    frequency &gt; 100 & docfreq &lt; 60\n  ) +\n  ggrepel::geom_text_repel(aes(label = feature)) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"文書数\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "words-menu-1.html#kwica.5.5",
    "href": "words-menu-1.html#kwica.5.5",
    "title": "2  抽出語メニュー1",
    "section": "2.5 KWIC（A.5.5）",
    "text": "2.5 KWIC（A.5.5）\n\n2.5.1 コンコーダンス\nコンコーダンスはquanteda::kwic()で確認できる。もっとも、KH Coderの提供するコンコーダンス検索やコロケーション統計のほうが明らかにリッチなのと、Rのコンソールは日本語の長めの文字列を表示するのにあまり向いていないというのがあるので、このあたりの機能が必要ならKH Coderを利用したほうがよい。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(section == \"[1]上_先生と私\") |&gt;\n  dplyr::select(label, token) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::reframe(token = list(token), .by = label) |&gt;\n  tibble::deframe() |&gt;\n  quanteda::as.tokens() |&gt;\n  quanteda::tokens_select(\"[[:punct:]]\", selection = \"remove\", valuetype = \"regex\", padding = FALSE) |&gt;\n  quanteda::tokens_select(\"^[\\\\p{Hiragana}]{1,2}$\", selection = \"remove\", valuetype = \"regex\", padding = TRUE)\n\nquanteda::kwic(dat, pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\")\n#&gt; Keyword-in-context with 18 matches.                                                                   \n#&gt;      [上・二, 183]            海 方 | 向い | 立っ                  \n#&gt;      [上・二, 527]            沖 方 | 向い | 行っ それから 引き返し\n#&gt;      [上・七, 740]            外 方 | 向い | 今 手                 \n#&gt;      [上・八, 622]            私 方 | 向い | 私                    \n#&gt;      [上・八, 697]            私 方 | 向い | 子供                  \n#&gt;    [上・十二, 568]        花 そちら | 向い | 眼 峙                 \n#&gt;    [上・十二, 628]          方角 足 | 向け | それから 私           \n#&gt;    [上・十四, 247]            庭 方 | 向い | 庭 この間             \n#&gt;    [上・十五, 661]      奥さん 差し | 向い | 話 なけれ             \n#&gt;    [上・十六, 265]            眼 私 | 向け | そうして 客 来        \n#&gt;    [上・十七, 447]    世の中 どっち | 向い | 面白                  \n#&gt;  [上・二十五, 557]        世間 背中 | 向け | 人 苦味               \n#&gt;  [上・二十六, 480]                  | 向い | 歩い やがて 若葉      \n#&gt;  [上・三十三, 686]            庭 方 | 向い | 澄まし 烟草           \n#&gt;  [上・三十三, 771] 先生 ちょっと 顔 | 向け | 直し 奥さん 言葉      \n#&gt;  [上・三十四, 415]               下 | 向い | 私 父                 \n#&gt;  [上・三十四, 436]   突然 奥さん 方 | 向い | 静 お前               \n#&gt;  [上・三十五, 425]            庭 方 | 向い | 笑っ しかし\n\n\nなお、上の例ではひらがな1～2文字の語をpaddingしつつ除外したので、一部の助詞などは表示されていない（それぞれの窓のなかでトークンとして数えられてはいる）。\n\n\n2.5.2 コロケーション\nたとえば、前後5個のwindow内のコロケーション（nodeを含めて11語の窓ということ）の合計については次のように確認できる。\n\n\nCode\ndat |&gt;\n  quanteda::fcm(context = \"window\", window = 5) |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(node = document, term = term) |&gt;\n  dplyr::filter(node == \"向い\") |&gt;\n  dplyr::slice_max(count, n = 10)\n#&gt; # A tibble: 30 × 3\n#&gt;    node  term     count\n#&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 向い  庭           4\n#&gt;  2 向い  奥さん       2\n#&gt;  3 向い  立っ         1\n#&gt;  4 向い  眼           1\n#&gt;  5 向い  やがて       1\n#&gt;  6 向い  沖           1\n#&gt;  7 向い  それから     1\n#&gt;  8 向い  引き返し     1\n#&gt;  9 向い  烟草         1\n#&gt; 10 向い  しかし       1\n#&gt; # ℹ 20 more rows\n\n\n「左合計」や「右合計」については、たとえば次のようにして確認できる。paddingしなければtidyr::separate_wider_delim()で展開して位置ごとに集計することもできそう。\n\n\nCode\ndat |&gt;\n  quanteda::kwic(pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\") |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::select(docname, keyword, pre, post) |&gt;\n  tidyr::pivot_longer(\n    c(pre, post),\n    names_to = \"window\",\n    values_to = \"term\",\n    values_transform = ~ strsplit(., \" \", fixed = TRUE)\n  ) |&gt;\n  tidyr::unnest(term) |&gt;\n  dplyr::count(window, term, sort = TRUE)\n#&gt; # A tibble: 55 × 3\n#&gt;    window term         n\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;int&gt;\n#&gt;  1 pre    方           9\n#&gt;  2 post   私           3\n#&gt;  3 pre    庭           3\n#&gt;  4 pre    私           3\n#&gt;  5 post   それから     2\n#&gt;  6 pre    奥さん       2\n#&gt;  7 post   お前         1\n#&gt;  8 post   この間       1\n#&gt;  9 post   しかし       1\n#&gt; 10 post   そうして     1\n#&gt; # ℹ 45 more rows\n\n\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package            * version    date (UTC) lib source\n#&gt;  audubon              0.5.1.9001 2023-09-03 [1] https://paithiov909.r-universe.dev (R 4.3.1)\n#&gt;  blob                 1.2.4      2023-03-17 [1] RSPM (R 4.3.0)\n#&gt;  cachem               1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  cellranger           1.1.0      2016-07-27 [1] RSPM (R 4.3.0)\n#&gt;  cli                  3.6.2      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  colorspace           2.1-0      2023-01-23 [1] RSPM (R 4.3.0)\n#&gt;  crosstalk            1.2.1      2023-11-23 [1] RSPM (R 4.3.0)\n#&gt;  curl                 5.2.1      2024-03-01 [1] CRAN (R 4.3.3)\n#&gt;  DBI                * 1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  dbplyr               2.4.0      2023-10-26 [1] RSPM (R 4.3.0)\n#&gt;  digest               0.6.34     2024-01-11 [1] CRAN (R 4.3.2)\n#&gt;  dplyr                1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  duckdb             * 0.9.2-1    2023-11-28 [1] RSPM (R 4.3.2)\n#&gt;  ellipsis             0.3.2      2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;  evaluate             0.23       2023-11-01 [1] CRAN (R 4.3.1)\n#&gt;  fansi                1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n#&gt;  farver               2.1.1      2022-07-06 [1] RSPM (R 4.3.0)\n#&gt;  fastmap              1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fastmatch            1.1-4      2023-08-18 [1] RSPM (R 4.3.0)\n#&gt;  generics             0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2            * 3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  gibasa               1.1.0      2024-02-17 [1] https://paithiov909.r-universe.dev (R 4.3.2)\n#&gt;  glue                 1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n#&gt;  gtable               0.3.4      2023-08-21 [1] CRAN (R 4.3.1)\n#&gt;  htmltools            0.5.7      2023-11-03 [1] RSPM (R 4.3.0)\n#&gt;  htmlwidgets          1.6.4      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  httpgd               1.3.1      2023-01-30 [1] CRAN (R 4.3.0)\n#&gt;  janeaustenr          1.0.0      2022-08-26 [1] RSPM (R 4.3.0)\n#&gt;  jsonlite             1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n#&gt;  knitr                1.45       2023-10-30 [1] CRAN (R 4.3.1)\n#&gt;  labeling             0.4.3      2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  later                1.3.2      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  lattice              0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  lifecycle            1.0.4      2023-11-07 [1] RSPM (R 4.3.0)\n#&gt;  magrittr             2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  Matrix               1.6-3      2023-11-14 [4] CRAN (R 4.3.2)\n#&gt;  memoise              2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  mgcv                 1.9-1      2023-12-21 [4] CRAN (R 4.3.2)\n#&gt;  munsell              0.5.0      2018-06-12 [1] RSPM (R 4.3.0)\n#&gt;  nlme                 3.1-163    2023-08-09 [4] CRAN (R 4.3.1)\n#&gt;  nsyllable            1.0.1      2022-02-28 [1] RSPM (R 4.3.0)\n#&gt;  pillar               1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig            2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  purrr                1.0.2      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  quanteda             3.3.1      2023-05-18 [1] RSPM (R 4.3.0)\n#&gt;  quanteda.textstats   0.96.4     2023-11-02 [1] CRAN (R 4.3.1)\n#&gt;  R.cache              0.16.0     2022-07-21 [1] CRAN (R 4.3.0)\n#&gt;  R.methodsS3          1.8.2      2022-06-13 [1] CRAN (R 4.3.0)\n#&gt;  R.oo                 1.26.0     2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  R.utils              2.12.3     2023-11-18 [1] CRAN (R 4.3.2)\n#&gt;  R6                   2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp                 1.0.12     2024-01-09 [1] RSPM (R 4.3.0)\n#&gt;  RcppParallel         5.1.7      2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;  reactable            0.4.4      2023-03-12 [1] RSPM (R 4.3.0)\n#&gt;  reactablefmtr        2.0.0      2022-03-16 [1] RSPM (R 4.3.2)\n#&gt;  reactR               0.5.0      2023-10-11 [1] RSPM (R 4.3.0)\n#&gt;  readxl               1.4.3      2023-07-06 [1] RSPM (R 4.3.0)\n#&gt;  rlang                1.1.3      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  rmarkdown            2.25       2023-09-18 [1] RSPM (R 4.3.0)\n#&gt;  sass                 0.4.8      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  scales               1.3.0      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  sessioninfo          1.2.2      2021-12-06 [1] RSPM (R 4.3.0)\n#&gt;  SnowballC            0.7.1      2023-04-25 [1] RSPM (R 4.3.0)\n#&gt;  stopwords            2.3        2021-10-28 [1] RSPM (R 4.3.0)\n#&gt;  stringi              1.8.3      2024-01-28 [1] https://gagolews.r-universe.dev (R 4.3.2)\n#&gt;  styler               1.10.2     2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  systemfonts          1.0.5      2023-10-09 [1] RSPM (R 4.3.0)\n#&gt;  tibble               3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyr                1.3.1      2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  tidyselect           1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidytext             0.4.1      2023-01-07 [1] RSPM (R 4.3.0)\n#&gt;  tokenizers           0.3.0      2022-12-22 [1] RSPM (R 4.3.0)\n#&gt;  utf8                 1.2.4      2023-10-22 [1] RSPM (R 4.3.0)\n#&gt;  V8                   4.4.2      2024-02-15 [1] CRAN (R 4.3.2)\n#&gt;  vctrs                0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#&gt;  withr                3.0.0      2024-01-16 [1] RSPM (R 4.3.0)\n#&gt;  xfun                 0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  yaml                 2.3.8      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.3\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "words-menu-2.html",
    "href": "words-menu-2.html",
    "title": "3  抽出語メニュー2",
    "section": "",
    "text": "3.1 関連語検索（A.5.6）",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "words-menu-2.html#関連語検索a.5.6",
    "href": "words-menu-2.html#関連語検索a.5.6",
    "title": "3  抽出語メニュー2",
    "section": "",
    "text": "3.1.1 関連語のリスト\n「確率差」や「確率比」については、いちおう計算はできた気がするが、あっているのかよくわからない。また、このやり方はそれなりの数の共起について計算をしなければならず、共起行列が大きくなると大変そう。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    section == \"[1]上_先生と私\",\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\ndat &lt;- dfm |&gt;\n  quanteda::fcm() |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(target = document, co_occur = count) |&gt;\n  dplyr::reframe(\n    term = term,\n    target_occur = quanteda::colSums(dfm)[target],\n    term_occur = quanteda::colSums(dfm)[term],\n    co_occur = co_occur,\n    .by = target\n  ) |&gt;\n  dplyr::mutate(\n    p_x = target_occur / quanteda::ndoc(dfm),\n    p_y = term_occur / quanteda::ndoc(dfm),\n    p_xy = (co_occur / quanteda::ndoc(dfm)) / p_x,\n    differential = p_xy - p_y, # 確率差\n    lift = p_xy / p_y, # 確率比（リフト）,\n    jaccard = co_occur / (target_occur + term_occur - co_occur),\n    dice = 2 * co_occur / (target_occur + term_occur)\n  ) |&gt;\n  dplyr::select(target, term, differential, lift, jaccard, dice)\n\ndat\n#&gt; # A tibble: 26,820 × 6\n#&gt;    target    term        differential   lift jaccard   dice\n#&gt;    &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 書く/動詞 心持/名詞         0.0553  3.56   0.0417 0.08  \n#&gt;  2 書く/動詞 建てる/動詞       0.0733 21.4    0.0714 0.133 \n#&gt;  3 書く/動詞 地位/名詞         0.0733 21.4    0.0714 0.133 \n#&gt;  4 書く/動詞 来る/動詞         0.0819  2.14   0.0392 0.0755\n#&gt;  5 書く/動詞 一言/名詞         0.0733 21.4    0.0714 0.133 \n#&gt;  6 書く/動詞 見る/動詞         0.121   2.10   0.0423 0.0811\n#&gt;  7 書く/動詞 思う/動詞         0.182   2.44   0.0506 0.0964\n#&gt;  8 書く/動詞 出る/動詞        -0.0166  0.822  0.0156 0.0308\n#&gt;  9 書く/動詞 先生/名詞         0.0618  1.13   0.0258 0.0504\n#&gt; 10 書く/動詞 帰る/動詞         0.309   5.09   0.1    0.182 \n#&gt; # ℹ 26,810 more rows\n\n\n\n\n3.1.2 共起ネットワーク\n「先生/名詞」と関連の強そうな語の共起を図示した例。\n「先生/名詞」と共起している語のうち、出現回数が上位20位以内である語がtargetである共起を抽出したうえで、それらのなかからJaccard係数が大きい順に75個だけ残している。「先生/名詞」という語そのものは図に含めていない。\n\n\nCode\ndat |&gt;\n  dplyr::inner_join(\n    dplyr::filter(dat, target == \"先生/名詞\") |&gt; dplyr::select(term),\n    by = dplyr::join_by(target == term)\n  ) |&gt;\n  dplyr::filter(target %in% names(quanteda::topfeatures(dfm, 20))) |&gt;\n  dplyr::slice_max(jaccard, n = 75) |&gt;\n  tidygraph::as_tbl_graph(directed = FALSE) |&gt;\n  tidygraph::to_minimum_spanning_tree() |&gt;\n  purrr::pluck(\"mst\") |&gt;\n  dplyr::mutate(\n    community = factor(tidygraph::group_leading_eigen())\n  ) |&gt;\n  ggraph::ggraph(layout = \"fr\") +\n  ggraph::geom_edge_link(aes(width = sqrt(lift), alpha = jaccard)) +\n  ggraph::geom_node_point(aes(colour = community), show.legend = FALSE) +\n  ggraph::geom_node_text(aes(label = name, colour = community), repel = TRUE, show.legend = FALSE) +\n  ggraph::theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 アソシエーション分析🍳\n英語だとこのメニューの名前は「Word Association」となっているので、ふつうにアソシエーション分析すればいいと思った。\narulesのtransactionsオブジェクトをつくるには、quantedaのfcmオブジェクトから変換すればよい（arulesをアタッチしている必要がある）。\n\n\nCode\nlibrary(arules)\nlibrary(arulesViz)\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  quanteda::fcm() |&gt;\n  as(\"nMatrix\") |&gt;\n  as(\"transactions\")\n\n\narules::apriori()でアソシエーションルールを抽出する。\n\n\nCode\nrules &lt;-\n  arules::apriori(\n    dat,\n    parameter = list(\n      support = 0.075,\n      confidence = 0.8,\n      minlen = 2,\n      maxlen = 2, # LHS+RHSの長さ。変えないほうがよい\n      maxtime = 5\n    ),\n    control = list(verbose = FALSE)\n  )\n\n\nこの形式のオブジェクトはas(rules, \"data.frame\")のようにしてデータフレームに変換できる。tibbleにしたい場合には次のようにすればよい。\n\n\nCode\nas(rules, \"data.frame\") |&gt;\n  dplyr::mutate(across(where(is.numeric), ~ signif(., digits = 3))) |&gt;\n  tidyr::separate_wider_delim(rules, delim = \" =&gt; \", names = c(\"lhs\", \"rhs\")) |&gt;\n  dplyr::arrange(desc(lift))\n#&gt; # A tibble: 65 × 7\n#&gt;    lhs           rhs         support confidence coverage  lift count\n#&gt;    &lt;chr&gt;         &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 {手紙/名詞}   {書く/動詞}  0.127       0.821   0.155   4.14   375\n#&gt;  2 {向う/動詞}   {K/未知語}   0.0839      0.832   0.101   2.43   247\n#&gt;  3 {掛ける/動詞} {聞く/動詞}  0.0754      0.835   0.0904  2.13   222\n#&gt;  4 {黙る/動詞}   {聞く/動詞}  0.103       0.83    0.124   2.12   303\n#&gt;  5 {始める/動詞} {聞く/動詞}  0.0788      0.806   0.0978  2.05   232\n#&gt;  6 {掛ける/動詞} {出る/動詞}  0.0768      0.85    0.0904  1.88   226\n#&gt;  7 {過ぎる/動詞} {自分/名詞}  0.0778      0.898   0.0866  1.87   229\n#&gt;  8 {考え/名詞}   {自分/名詞}  0.0798      0.88    0.0907  1.83   235\n#&gt;  9 {ぎり/未知語} {出る/動詞}  0.0761      0.827   0.0921  1.83   224\n#&gt; 10 {始める/動詞} {出る/動詞}  0.0791      0.809   0.0978  1.79   233\n#&gt; # ℹ 55 more rows\n\n\n\n\n3.1.4 散布図🍳\n\n\nCode\nplot(rules, engine = \"html\")\n#&gt; To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n3.1.5 バルーンプロット🍳\n\n\nCode\nplot(rules, method = \"grouped\", engine = \"html\")\n\n\n\n\n\n\n\n\n3.1.6 ネットワーク図🍳\n\n\nCode\nplot(rules, method = \"graph\", engine = \"html\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "words-menu-2.html#対応分析a.5.7",
    "href": "words-menu-2.html#対応分析a.5.7",
    "title": "3  抽出語メニュー2",
    "section": "3.2 対応分析（A.5.7）",
    "text": "3.2 対応分析（A.5.7）\n\n3.2.1 コレスポンデンス分析\n段落（doc_id）内の頻度で語彙を削ってから部（section）ごとに集計するために、ややめんどうなことをしている。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(\n    min_termfreq = 75,\n    termfreq_type = \"rank\",\n    min_docfreq = 30,\n    docfreq_type = \"count\"\n  )\n\n\nこうしてdoc_idごとに集計したdfmオブジェクトを一度tidytext::tidy()して3つ組のデータフレームに戻し、sectionのラベルを結合する。このデータフレームをもう一度tidytext::cast_dfm()で疎行列に変換して、quanteda.textmodels::textmodel_ca()を使って対応分析にかける。\n\n\nCode\nca_fit &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  tidytext::cast_dfm(section, term, count) |&gt;\n  quanteda.textmodels::textmodel_ca(nd = 2, sparse = TRUE)\n\n\nこの関数は疎行列に対して計算をおこなえるため、比較的大きな行列を渡しても大丈夫。\n\n\n3.2.2 バイプロット\ncaパッケージを読み込んでいるとplot()でバイプロットを描ける。factoextra::fviz_ca_biplot()でも描けるが、見た目はplot()のとあまり変わらない。\n\n\nCode\nlibrary(ca)\ndat &lt;- plot(ca_fit)\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 バイプロット（バブルプロット）\nggplot2でバイプロットを描画するには、たとえば次のようにする。ggrepel::geom_text_repelでラベルを出す語彙の選択の仕方はもうすこし工夫したほうがよいかもしれない。\nなお、このコードはCorrespondence Analysis visualization using ggplot | R-bloggersを参考にした。\n\n\nCode\ntf &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  dplyr::summarise(tf = sum(count), .by = term) |&gt;\n  dplyr::pull(tf, term)\n\n# modified from https://www.r-bloggers.com/2019/08/correspondence-analysis-visualization-using-ggplot/\nmake_ca_plot_df &lt;- function(ca.plot.obj, row.lab = \"Rows\", col.lab = \"Columns\") {\n  tibble::tibble(\n    Label = c(\n      rownames(ca.plot.obj$rows),\n      rownames(ca.plot.obj$cols)\n    ),\n    Dim1 = c(\n      ca.plot.obj$rows[, 1],\n      ca.plot.obj$cols[, 1]\n    ),\n    Dim2 = c(\n      ca.plot.obj$rows[, 2],\n      ca.plot.obj$cols[, 2]\n    ),\n    Variable = c(\n      rep(row.lab, nrow(ca.plot.obj$rows)),\n      rep(col.lab, nrow(ca.plot.obj$cols))\n    )\n  )\n}\ndat &lt;- dat |&gt;\n  make_ca_plot_df(row.lab = \"Construction\", col.lab = \"Medium\") |&gt;\n  dplyr::mutate(\n    Size = dplyr::if_else(Variable == \"Construction\", mean(tf), tf[Label])\n  )\n# 非ASCII文字のラベルに対してwarningを出さないようにする\nsuppressWarnings({\n  ca_sum &lt;- summary(ca_fit)\n  dim_var_percs &lt;- ca_sum$scree[, \"values2\"]\n})\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, col = Variable, label = Label)) +\n  geom_vline(xintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_hline(yintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_jitter(aes(size = Size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_label_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Construction\"),\n    show.legend = FALSE\n  ) +\n  ggrepel::geom_text_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Medium\", sqrt(Dim1^2 + Dim2^2) &gt; 0.25),\n    show.legend = FALSE\n  ) +\n  scale_x_continuous(\n    limits = range(dat$Dim1) +\n      c(diff(range(dat$Dim1)) * -0.2, diff(range(dat$Dim1)) * 0.2)\n  ) +\n  scale_y_continuous(\n    limits = range(dat$Dim2) +\n      c(diff(range(dat$Dim2)) * -0.2, diff(range(dat$Dim2)) * 0.2)\n  ) +\n  scale_size_area(max_size = 16) +\n  labs(\n    x = paste0(\"Dimension 1 (\", signif(dim_var_percs[1], 3), \"%)\"),\n    y = paste0(\"Dimension 2 (\", signif(dim_var_percs[2], 3), \"%)\")\n  ) +\n  theme_classic()\n#&gt; Warning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\n#&gt; increasing max.overlaps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "words-menu-2.html#多次元尺度構成法a.5.8",
    "href": "words-menu-2.html#多次元尺度構成法a.5.8",
    "title": "3  抽出語メニュー2",
    "section": "3.3 多次元尺度構成法（A.5.8）",
    "text": "3.3 多次元尺度構成法（A.5.8）\n\n3.3.1 MDS・バブルプロット\nMASS::isoMDSよりMASS::sammonのほうがたぶん見やすい。\n\n\nCode\nsimil &lt;- dfm |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\")\n\ndat &lt;- MASS::sammon(1 - simil, k = 2) |&gt;\n  purrr::pluck(\"points\")\n#&gt; Initial stress        : 0.62279\n#&gt; stress after   0 iters: 0.62279\n\n\n\n\nCode\ndat &lt;- dat |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"label\") |&gt;\n  dplyr::rename(Dim1 = V1, Dim2 = V2) |&gt;\n  dplyr::mutate(\n    size = tf[label],\n    clust = (hclust(\n      proxyC::dist(dat, method = \"euclidean\") |&gt; as.dist(),\n      method = \"ward.D2\"\n    ) |&gt; cutree(k = 6))[label]\n  )\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, label = label, col = factor(clust))) +\n  geom_point(aes(size = size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_text_repel(show.legend = FALSE) +\n  scale_size_area(max_size = 16) +\n  theme_classic()\n#&gt; Warning: ggrepel: 23 unlabeled data points (too many overlaps). Consider\n#&gt; increasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 MDS・3次元プロット\nscatterplot3dではなくplotlyで試してみたが、とくに見やすいということはなかったかもしれない。\n\n\nCode\ndat &lt;- MASS::sammon(1 - simil, k = 3) |&gt;\n  purrr::pluck(\"points\") |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"label\") |&gt;\n  dplyr::rename(Dim1 = V1, Dim2 = V2, Dim3 = V3) |&gt;\n  dplyr::mutate(term_freq = tf[label])\n#&gt; Initial stress        : 0.53925\n#&gt; stress after  10 iters: 0.18929, magic = 0.092\n#&gt; stress after  20 iters: 0.14417, magic = 0.043\n#&gt; stress after  21 iters: 0.13794\n\n\n\n\nCode\ndat |&gt;\n  plotly::plot_ly(x = ~Dim1, y = ~Dim2, z = ~Dim3, text = ~label, color = ~term_freq) |&gt;\n  plotly::add_markers(opacity = .5)\n\n\n\n\n\n\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version    date (UTC) lib source\n#&gt;  arules       * 1.7-7      2023-11-29 [1] RSPM (R 4.3.0)\n#&gt;  arulesViz    * 1.5-2      2023-03-07 [1] RSPM (R 4.3.0)\n#&gt;  audubon        0.5.1.9001 2023-09-03 [1] https://paithiov909.r-universe.dev (R 4.3.1)\n#&gt;  blob           1.2.4      2023-03-17 [1] RSPM (R 4.3.0)\n#&gt;  ca           * 0.71.1     2020-01-24 [1] RSPM (R 4.3.0)\n#&gt;  cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  cellranger     1.1.0      2016-07-27 [1] RSPM (R 4.3.0)\n#&gt;  cli            3.6.2      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  codetools      0.2-19     2023-02-01 [4] CRAN (R 4.2.2)\n#&gt;  colorspace     2.1-0      2023-01-23 [1] RSPM (R 4.3.0)\n#&gt;  crosstalk      1.2.1      2023-11-23 [1] RSPM (R 4.3.0)\n#&gt;  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.3)\n#&gt;  data.table     1.15.2     2024-02-29 [1] RSPM (R 4.3.0)\n#&gt;  DBI          * 1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  dbplyr         2.4.0      2023-10-26 [1] RSPM (R 4.3.0)\n#&gt;  digest         0.6.34     2024-01-11 [1] CRAN (R 4.3.2)\n#&gt;  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  duckdb       * 0.9.2-1    2023-11-28 [1] RSPM (R 4.3.2)\n#&gt;  ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.1)\n#&gt;  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n#&gt;  farver         2.1.1      2022-07-06 [1] RSPM (R 4.3.0)\n#&gt;  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fastmatch      1.1-4      2023-08-18 [1] RSPM (R 4.3.0)\n#&gt;  foreach        1.5.2      2022-02-02 [1] RSPM (R 4.3.0)\n#&gt;  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggforce        0.4.2      2024-02-19 [1] RSPM (R 4.3.0)\n#&gt;  ggplot2      * 3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  ggraph         2.2.0      2024-02-27 [1] CRAN (R 4.3.2)\n#&gt;  ggrepel        0.9.5      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  gibasa         1.1.0      2024-02-17 [1] https://paithiov909.r-universe.dev (R 4.3.2)\n#&gt;  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n#&gt;  graphlayouts   1.1.0      2024-01-19 [1] RSPM (R 4.3.0)\n#&gt;  gridExtra      2.3        2017-09-09 [1] RSPM (R 4.3.0)\n#&gt;  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.1)\n#&gt;  htmltools      0.5.7      2023-11-03 [1] RSPM (R 4.3.0)\n#&gt;  htmlwidgets    1.6.4      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  httpgd         1.3.1      2023-01-30 [1] CRAN (R 4.3.0)\n#&gt;  httr           1.4.7      2023-08-15 [1] RSPM (R 4.3.0)\n#&gt;  igraph         2.0.2      2024-02-17 [1] RSPM (R 4.3.0)\n#&gt;  iterators      1.0.14     2022-02-05 [1] RSPM (R 4.3.0)\n#&gt;  janeaustenr    1.0.0      2022-08-26 [1] RSPM (R 4.3.0)\n#&gt;  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n#&gt;  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.1)\n#&gt;  labeling       0.4.3      2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  later          1.3.2      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  lattice        0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  lazyeval       0.2.2      2019-03-15 [1] CRAN (R 4.3.0)\n#&gt;  lifecycle      1.0.4      2023-11-07 [1] RSPM (R 4.3.0)\n#&gt;  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  MASS           7.3-60     2023-05-04 [4] CRAN (R 4.3.1)\n#&gt;  Matrix       * 1.6-3      2023-11-14 [4] CRAN (R 4.3.2)\n#&gt;  memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  munsell        0.5.0      2018-06-12 [1] RSPM (R 4.3.0)\n#&gt;  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  plotly         4.10.4     2024-01-13 [1] RSPM (R 4.3.0)\n#&gt;  polyclip       1.10-6     2023-09-27 [1] RSPM (R 4.3.0)\n#&gt;  proxyC         0.3.4      2023-10-25 [1] RSPM (R 4.3.0)\n#&gt;  purrr          1.0.2      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  quanteda       3.3.1      2023-05-18 [1] RSPM (R 4.3.0)\n#&gt;  R.cache        0.16.0     2022-07-21 [1] CRAN (R 4.3.0)\n#&gt;  R.methodsS3    1.8.2      2022-06-13 [1] CRAN (R 4.3.0)\n#&gt;  R.oo           1.26.0     2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  R.utils        2.12.3     2023-11-18 [1] CRAN (R 4.3.2)\n#&gt;  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp           1.0.12     2024-01-09 [1] RSPM (R 4.3.0)\n#&gt;  RcppParallel   5.1.7      2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;  readxl         1.4.3      2023-07-06 [1] RSPM (R 4.3.0)\n#&gt;  registry       0.5-1      2019-03-05 [1] RSPM (R 4.3.0)\n#&gt;  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  rmarkdown      2.25       2023-09-18 [1] RSPM (R 4.3.0)\n#&gt;  scales         1.3.0      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  seriation      1.5.4      2023-12-12 [1] RSPM (R 4.3.0)\n#&gt;  sessioninfo    1.2.2      2021-12-06 [1] RSPM (R 4.3.0)\n#&gt;  SnowballC      0.7.1      2023-04-25 [1] RSPM (R 4.3.0)\n#&gt;  stopwords      2.3        2021-10-28 [1] RSPM (R 4.3.0)\n#&gt;  stringi        1.8.3      2024-01-28 [1] https://gagolews.r-universe.dev (R 4.3.2)\n#&gt;  stringr        1.5.1      2023-11-14 [1] RSPM (R 4.3.0)\n#&gt;  styler         1.10.2     2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  systemfonts    1.0.5      2023-10-09 [1] RSPM (R 4.3.0)\n#&gt;  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidygraph      1.3.1      2024-01-30 [1] RSPM (R 4.3.0)\n#&gt;  tidyr          1.3.1      2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidytext       0.4.1      2023-01-07 [1] RSPM (R 4.3.0)\n#&gt;  tokenizers     0.3.0      2022-12-22 [1] RSPM (R 4.3.0)\n#&gt;  TSP            1.2-4      2023-04-04 [1] RSPM (R 4.3.0)\n#&gt;  tweenr         2.0.3      2024-02-26 [1] CRAN (R 4.3.2)\n#&gt;  utf8           1.2.4      2023-10-22 [1] RSPM (R 4.3.0)\n#&gt;  V8             4.4.2      2024-02-15 [1] CRAN (R 4.3.2)\n#&gt;  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#&gt;  viridis        0.6.5      2024-01-29 [1] RSPM (R 4.3.0)\n#&gt;  viridisLite    0.4.2      2023-05-02 [1] RSPM (R 4.3.0)\n#&gt;  visNetwork     2.1.2      2022-09-29 [1] RSPM (R 4.3.0)\n#&gt;  withr          3.0.0      2024-01-16 [1] RSPM (R 4.3.0)\n#&gt;  xfun           0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  yaml           2.3.8      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.3\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "words-menu-3.html",
    "href": "words-menu-3.html",
    "title": "4  抽出語メニュー3",
    "section": "",
    "text": "4.1 階層的クラスター分析（A.5.9）",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "words-menu-3.html#階層的クラスター分析a.5.9",
    "href": "words-menu-3.html#階層的クラスター分析a.5.9",
    "title": "4  抽出語メニュー3",
    "section": "",
    "text": "4.1.1 非類似度のヒートマップ🍳\nJaccard係数を指定して非類似度のヒートマップを描くと、そもそもパターンがほとんど見えなかった。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\ndat &lt;- dfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = \"rank\") |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"dice\") |&gt;\n  rlang::as_function(~ 1 - .)()\n\nfactoextra::fviz_dist(as.dist(dat))\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 階層的クラスタリング\n\n\nCode\nclusters &lt;-\n  as.dist(dat) |&gt;\n  hclust(method = \"ward.D2\")\n\n\n\n\n4.1.3 シルエット分析🍳\n\n\nCode\nfactoextra::fviz_nbclust(\n  as.matrix(dat),\n  FUNcluster = factoextra::hcut,\n  k.max = ceiling(sqrt(nrow(dat)))\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncluster::silhouette(cutree(clusters, k = 5), dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic()\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n\n\n4.1.4 デンドログラム\nデンドログラムについては、似たような表現を手軽に実現できる方法が見つけられない。ラベルの位置が左右反転しているが、factoextra::fviz_dend(horiz = TRUE)とするのが簡単かもしれない。\n\n\nCode\nfactoextra::fviz_dend(clusters, k = 5, horiz = TRUE, labels_track_height = 0.3)\n#&gt; Warning in seq.default(15, 375, length = k + 1): 'length' の 'length.out'\n#&gt; への部分的な引数のマッチング\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\n4.1.5 デンドログラムと棒グラフ\nKH Coderのソースコードを見た感じ、デンドログラムと一緒に語の出現回数を描いている表現は、やや独特なことをしている。むしろ語の出現回数のほうが主な情報になってよいなら、ふつうの棒グラフの横にggh4x::scale_y_dendrogram()でデンドログラムを描くことができる。\n\n\nCode\ndfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = \"rank\") |&gt;\n  quanteda::colSums() |&gt;\n  tibble::enframe() |&gt;\n  dplyr::mutate(\n    clust = (clusters |&gt; cutree(k = 5))[name]\n  ) |&gt;\n  ggplot(aes(x = value, y = name, fill = factor(clust))) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_x_sqrt() +\n  ggh4x::scale_y_dendrogram(hclust = clusters) +\n  labs(x = \"出現回数\", y = element_blank()) +\n  theme_bw()\n#&gt; Warning: The S3 guide system was deprecated in ggplot2 3.5.0.\n#&gt; ℹ It has been replaced by a ggproto system that can be extended.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "words-menu-3.html#共起ネットワークa.5.10",
    "href": "words-menu-3.html#共起ネットワークa.5.10",
    "title": "4  抽出語メニュー3",
    "section": "4.2 共起ネットワーク（A.5.10）",
    "text": "4.2 共起ネットワーク（A.5.10）\n\n4.2.1 グラフの作成\n描画するグラフをtbl_graphとして作成する。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\ndat &lt;- dfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 45, termfreq_type = \"count\") |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\", rank = 3) |&gt;\n  as.matrix() |&gt;\n  tidygraph::as_tbl_graph(directed = FALSE) |&gt;\n  dplyr::distinct() |&gt; # 重複を削除\n  tidygraph::activate(edges) |&gt;\n  dplyr::filter(from != to)\n\ndat\n#&gt; # A tbl_graph: 47 nodes and 82 edges\n#&gt; #\n#&gt; # An undirected simple graph with 2 components\n#&gt; #\n#&gt; # Edge Data: 82 × 3 (active)\n#&gt;     from    to weight\n#&gt;    &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n#&gt;  1     1    15 0.139 \n#&gt;  2     1    22 0.137 \n#&gt;  3     2     3 0.102 \n#&gt;  4     2    21 0.107 \n#&gt;  5     2    29 0.0964\n#&gt;  6     2    45 0.0779\n#&gt;  7     3     4 0.106 \n#&gt;  8     3    10 0.125 \n#&gt;  9     3    13 0.0827\n#&gt; 10     3    21 0.18  \n#&gt; # ℹ 72 more rows\n#&gt; #\n#&gt; # Node Data: 47 × 1\n#&gt;   name     \n#&gt;   &lt;chr&gt;    \n#&gt; 1 先生/名詞\n#&gt; 2 取る/動詞\n#&gt; 3 思う/動詞\n#&gt; # ℹ 44 more rows\n\n\n\n\n4.2.2 相関係数の計算\nエッジのalphaに渡す相関係数を計算する。このあたりのコードは書くのがなかなか難しかったので、あまりスマートなやり方ではないかもしれない。\nKH Coderには、それぞれの共起が文書集合内のどのあたりの位置に出現したかを概観できるようにするために、共起ネットワーク中のエッジについて、共起の出現位置との相関係数によって塗り分ける機能がある。これを実現するには、まずそれぞれの文書について文書集合内での通し番号を振ったうえで、それぞれの文書についてエッジとして描きたい共起の有無を1, 0で表してから、通し番号とのあいだの相関係数を計算するということをやる。\nまず、共起ネットワーク中に描きこむ共起と、それらを含む文書番号をリストアップした縦長のデータフレームをつくる。\n\n\nCode\nnodes &lt;- tidygraph::activate(dat, nodes) |&gt; dplyr::pull(\"name\")\nfrom &lt;- nodes[tidygraph::activate(dat, edges) |&gt; dplyr::pull(\"from\")]\nto &lt;- nodes[tidygraph::activate(dat, edges) |&gt; dplyr::pull(\"to\")]\n\nhas_coocurrences &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::filter(token %in% nodes) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::reframe(\n    from = from,\n    to = to,\n    has_from = purrr::map_lgl(from, ~ . %in% token),\n    has_to = purrr::map_lgl(to, ~ . %in% token),\n    .by = doc_id\n  ) |&gt;\n  dplyr::filter(has_from & has_to) |&gt;\n  dplyr::group_by(from, to) |&gt;\n  dplyr::reframe(doc_id = doc_id)\n\nhas_coocurrences\n#&gt; # A tibble: 2,164 × 3\n#&gt;    from          to       doc_id\n#&gt;    &lt;chr&gt;         &lt;chr&gt;     &lt;int&gt;\n#&gt;  1 お嬢さん/名詞 K/未知語   1034\n#&gt;  2 お嬢さん/名詞 K/未知語   1035\n#&gt;  3 お嬢さん/名詞 K/未知語   1041\n#&gt;  4 お嬢さん/名詞 K/未知語   1042\n#&gt;  5 お嬢さん/名詞 K/未知語   1045\n#&gt;  6 お嬢さん/名詞 K/未知語   1046\n#&gt;  7 お嬢さん/名詞 K/未知語   1048\n#&gt;  8 お嬢さん/名詞 K/未知語   1049\n#&gt;  9 お嬢さん/名詞 K/未知語   1052\n#&gt; 10 お嬢さん/名詞 K/未知語   1054\n#&gt; # ℹ 2,154 more rows\n\n\n次に、このデータフレームを共起ごとにグルーピングして、共起の有無と通し番号とのあいだの相関係数を含むデータフレームをつくる。\n\n\nCode\ncorrelations &lt;- has_coocurrences |&gt;\n  dplyr::group_by(from, to) |&gt;\n  dplyr::group_map(\\(.x, .y) {\n    tibble::tibble(\n      doc_number = seq_len(nrow(tbl)),\n      from = which(nodes == .y$from),\n      to = which(nodes == .y$to)\n    ) |&gt;\n      dplyr::group_by(from, to) |&gt;\n      dplyr::reframe(\n        cor = cor(doc_number, as.numeric(doc_number %in% .x[[\"doc_id\"]]))\n      )\n  }) |&gt;\n  purrr::list_rbind()\n\ncorrelations\n#&gt; # A tibble: 82 × 3\n#&gt;     from    to     cor\n#&gt;    &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1    43    46  0.292 \n#&gt;  2    43    44  0.141 \n#&gt;  3    10    15  0.104 \n#&gt;  4    10    34  0.0467\n#&gt;  5    20    46  0.246 \n#&gt;  6    20    43  0.199 \n#&gt;  7    32    43  0.158 \n#&gt;  8    32    41  0.104 \n#&gt;  9     8    17 -0.176 \n#&gt; 10     8    19 -0.127 \n#&gt; # ℹ 72 more rows\n\n\n最後に、相関係数をtbl_graphのエッジと結合する。\n\n\nCode\ndat &lt;- dat |&gt;\n  tidygraph::activate(edges) |&gt;\n  dplyr::left_join(correlations, by = dplyr::join_by(from == from, to == to))\n\n\n\n\n4.2.3 共起ネットワーク\n上の処理が間違っていなければ、文書集合の後のほうによく出てくる共起であるほど、エッジの色が濃くなっているはず。\n\n\nCode\ndat |&gt;\n  tidygraph::activate(nodes) |&gt;\n  dplyr::mutate(\n    community = factor(tidygraph::group_leading_eigen())\n  ) |&gt;\n  ggraph::ggraph(layout = \"fr\") +\n  ggraph::geom_edge_link2(\n    aes(\n      alpha = dplyr::percent_rank(cor) + .01, # パーセンタイルが0だと透明になってしまうので、適当に下駄をはかせる\n      width = dplyr::percent_rank(weight) + 1\n    ),\n    colour = \"red\"\n  ) +\n  ggraph::geom_node_point(aes(colour = community), show.legend = FALSE) +\n  ggraph::geom_node_label(aes(colour = community, label = name), repel = TRUE, show.legend = FALSE) +\n  ggraph::theme_graph()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "words-menu-3.html#自己組織化マップa.5.11",
    "href": "words-menu-3.html#自己組織化マップa.5.11",
    "title": "4  抽出語メニュー3",
    "section": "4.3 自己組織化マップ（A.5.11）",
    "text": "4.3 自己組織化マップ（A.5.11）\n\n4.3.1 自己組織化マップ（SOM）\nSOMの実装としては、KH Coderはsomを使っているようだが、kohonenを使ったほうがよい。\n行列が非常に大きい場合にはkohonen::som(mode = \"online\")としてもよいのかもしれないが、一般にバッチ型のほうが収束が早く、数十ステップ程度回せば十分とされる。\n与える単語文書行列は、ここではtidytext::bind_tf_idf()を使ってTF-IDFで重みづけし、上位100語ほど抽出する。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::bind_tf_idf(token, doc_id, n) |&gt;\n  tidytext::cast_dfm(doc_id, token, tf_idf) |&gt;\n  quanteda::dfm_trim(\n    min_termfreq = 100,\n    termfreq_type = \"rank\"\n  ) |&gt;\n  as.matrix() |&gt;\n  scale() |&gt;\n  t()\n\nsom_fit &lt;-\n  kohonen::som(\n    dat,\n    grid = kohonen::somgrid(20, 16, \"hexagonal\"),\n    rlen = 50, # 学習回数\n    alpha = c(0.05, 0.01),\n    radius = 8,\n    dist.fcts = \"sumofsquares\",\n    mode = \"batch\",\n    init = aweSOM::somInit(dat, 20, 16)\n  )\n#&gt; Warning in seq.default(along = data): 'along' の 'along.with'\n#&gt; への部分的な引数のマッチング\n#&gt; Warning in seq.default(along = whatmap): 'along' の 'along.with'\n#&gt; への部分的な引数のマッチング\n#&gt; Warning in seq.default(along = data): 'along' の 'along.with'\n#&gt; への部分的な引数のマッチング\n\n\n\n\nCode\naweSOM::somQuality(som_fit, dat)\n#&gt; \n#&gt; ## Quality measures:\n#&gt;  * Quantization error     :  83.9693 \n#&gt;  * (% explained variance) :  92.52 \n#&gt;  * Topographic error      :  0.36 \n#&gt;  * Kaski-Lagus error      :  21.49617 \n#&gt;  \n#&gt; ## Number of obs. per map cell:\n#&gt;   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n#&gt;   0   0   1   0   1   0   0   0   0   1   0   0   1   0   0   1   0   0   1   1 \n#&gt;  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n#&gt;   1   0   0   1   0   0   0   0   1   0   0   0   0   0   0   1   0   0   1   0 \n#&gt;  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n#&gt;   0   0   1   0   0   0   1   0   0   0   0   0   1   0   0   1   0   0   0   1 \n#&gt;  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n#&gt;   1   0   0   0   1   0   0   1   0   0   2   0   0   1   1   0   0   0   2   0 \n#&gt;  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n#&gt;   0   0   1   0   1   0   0   0   1   1   0   1   0   0   0   0   0   1   0   1 \n#&gt; 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n#&gt;   1   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0 \n#&gt; 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n#&gt;   0   0   0   0   0   0   1   0   0   0   0   0   0   1   0   0   0   0   0   1 \n#&gt; 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n#&gt;   1   0   0   1   0   2   0   1   0   1   1   0   1   1   1   0   0   0   2   0 \n#&gt; 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n#&gt;   0   0   0   1   0   0   1   0   1   0   0   0   0   0   1   0   0   0   1   0 \n#&gt; 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n#&gt;   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   0 \n#&gt; 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n#&gt;   0   1   1   0   0   1   0   0   0   0   1   0   1   0   0   0   0   0   0   1 \n#&gt; 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n#&gt;   1   0   1   0   0   0   1   0   0   1   0   0   0   1   0   0   0   1   0   0 \n#&gt; 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n#&gt;   0   0   0   1   0   0   0   0   1   1   0   0   1   0   0   0   1   0   0   2 \n#&gt; 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n#&gt;   1   0   0   0   1   2   0   0   0   2   0   0   0   0   1   0   0   1   0   0 \n#&gt; 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n#&gt;   0   1   0   0   1   0   0   1   0   0   0   0   0   1   0   0   1   0   0   0 \n#&gt; 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n#&gt;   2   0   1   0   0   0   0   0   1   0   0   1   0   0   0   1   0   0   0   1\n\n\n\n\n4.3.2 U-Matrix\nU-matrixは「各ノードの参照ベクトルが近傍ノードと異なる度合いで色づけする方法」（自己組織化マップ入門）。暖色の箇所はデータ密度が低い「山間部」で、寒色の箇所はデータ密度が高い「平野部」みたいなイメージ、写像の勾配が急峻になっている箇所を境にしてクラスタが分かれていると判断するみたいな見方をする。\n\n\nCode\naweSOM::aweSOMsmoothdist(som_fit)\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n\n\nCode\naweSOM::aweSOMplot(\n  som_fit,\n  data = dat,\n  type = \"UMatrix\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 ヒットマップ🍳\n色を付けるためのクラスタリングをしておく。一部の「山間部」や「盆地」がクラスタになって、後はその他の部分みたいな感じに分かれるようだが、解釈するのに便利な感じで分かれてはくれなかったりする。\n\n\nCode\nclusters &lt;- som_fit |&gt;\n  purrr::pluck(\"codes\", 1) |&gt; # 参照ベクトル（codebook vectors）は`codes`にリストとして格納されている\n  dist() |&gt;\n  hclust(method = \"ward.D2\") |&gt;\n  cutree(k = 10)\n\n\nヒットマップ（hitmap, proportion map）は以下のような可視化の方法。ノードの中の六角形は各ノードが保持する参照ベクトルの数（比率）を表している。ノードの背景色が上のコードで得たクラスタに対応する。\n\n\nCode\naweSOM::aweSOMplot(\n  som_fit,\n  data = dat,\n  type = \"Hitmap\",\n  superclass = clusters\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version    date (UTC) lib source\n#&gt;  abind          1.4-5      2016-07-21 [1] RSPM (R 4.3.0)\n#&gt;  audubon        0.5.1.9001 2023-09-03 [1] https://paithiov909.r-universe.dev (R 4.3.1)\n#&gt;  aweSOM         1.3        2022-08-30 [1] RSPM (R 4.3.0)\n#&gt;  backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#&gt;  blob           1.2.4      2023-03-17 [1] RSPM (R 4.3.0)\n#&gt;  broom          1.0.5      2023-06-09 [1] RSPM (R 4.3.0)\n#&gt;  cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  car            3.1-2      2023-03-30 [1] RSPM (R 4.3.0)\n#&gt;  carData        3.0-5      2022-01-06 [1] RSPM (R 4.3.0)\n#&gt;  cellranger     1.1.0      2016-07-27 [1] RSPM (R 4.3.0)\n#&gt;  class          7.3-22     2023-05-03 [4] CRAN (R 4.3.1)\n#&gt;  cli            3.6.2      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  cluster        2.1.6      2023-12-01 [4] CRAN (R 4.3.2)\n#&gt;  colorspace     2.1-0      2023-01-23 [1] RSPM (R 4.3.0)\n#&gt;  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.3)\n#&gt;  DBI          * 1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  dbplyr         2.4.0      2023-10-26 [1] RSPM (R 4.3.0)\n#&gt;  dendextend     1.17.1     2023-03-25 [1] RSPM (R 4.3.0)\n#&gt;  digest         0.6.34     2024-01-11 [1] CRAN (R 4.3.2)\n#&gt;  dotCall64      1.1-1      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  duckdb       * 0.9.2-1    2023-11-28 [1] RSPM (R 4.3.2)\n#&gt;  e1071          1.7-14     2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.1)\n#&gt;  factoextra     1.0.7      2020-04-01 [1] RSPM (R 4.3.2)\n#&gt;  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n#&gt;  farver         2.1.1      2022-07-06 [1] RSPM (R 4.3.0)\n#&gt;  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fastmatch      1.1-4      2023-08-18 [1] RSPM (R 4.3.0)\n#&gt;  fields         15.2       2023-08-17 [1] RSPM (R 4.3.0)\n#&gt;  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggdendro       0.2.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  ggh4x          0.2.8      2024-01-23 [1] RSPM (R 4.3.0)\n#&gt;  ggplot2      * 3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  ggpubr         0.6.0      2023-02-10 [1] RSPM (R 4.3.0)\n#&gt;  ggrepel        0.9.5      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  ggsignif       0.6.4      2022-10-13 [1] RSPM (R 4.3.0)\n#&gt;  gibasa         1.1.0      2024-02-17 [1] https://paithiov909.r-universe.dev (R 4.3.2)\n#&gt;  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n#&gt;  gridExtra      2.3        2017-09-09 [1] RSPM (R 4.3.0)\n#&gt;  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.1)\n#&gt;  htmltools      0.5.7      2023-11-03 [1] RSPM (R 4.3.0)\n#&gt;  htmlwidgets    1.6.4      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  httpgd         1.3.1      2023-01-30 [1] CRAN (R 4.3.0)\n#&gt;  httpuv         1.6.14     2024-01-26 [1] RSPM (R 4.3.0)\n#&gt;  igraph         2.0.2      2024-02-17 [1] RSPM (R 4.3.0)\n#&gt;  isoband        0.2.7      2022-12-20 [1] RSPM (R 4.3.0)\n#&gt;  janeaustenr    1.0.0      2022-08-26 [1] RSPM (R 4.3.0)\n#&gt;  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n#&gt;  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.1)\n#&gt;  kohonen        3.0.12     2023-06-09 [1] RSPM (R 4.3.0)\n#&gt;  labeling       0.4.3      2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  later          1.3.2      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  lattice        0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  lifecycle      1.0.4      2023-11-07 [1] RSPM (R 4.3.0)\n#&gt;  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  maps           3.4.2      2023-12-15 [1] RSPM (R 4.3.0)\n#&gt;  MASS           7.3-60     2023-05-04 [4] CRAN (R 4.3.1)\n#&gt;  Matrix         1.6-3      2023-11-14 [4] CRAN (R 4.3.2)\n#&gt;  memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  mime           0.12       2021-09-28 [1] CRAN (R 4.3.0)\n#&gt;  munsell        0.5.0      2018-06-12 [1] RSPM (R 4.3.0)\n#&gt;  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  plyr           1.8.9      2023-10-02 [1] RSPM (R 4.3.0)\n#&gt;  promises       1.2.1      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  proxy          0.4-27     2022-06-09 [1] RSPM (R 4.3.0)\n#&gt;  proxyC         0.3.4      2023-10-25 [1] RSPM (R 4.3.0)\n#&gt;  purrr          1.0.2      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  quanteda       3.3.1      2023-05-18 [1] RSPM (R 4.3.0)\n#&gt;  R.cache        0.16.0     2022-07-21 [1] CRAN (R 4.3.0)\n#&gt;  R.methodsS3    1.8.2      2022-06-13 [1] CRAN (R 4.3.0)\n#&gt;  R.oo           1.26.0     2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  R.utils        2.12.3     2023-11-18 [1] CRAN (R 4.3.2)\n#&gt;  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  RColorBrewer   1.1-3      2022-04-03 [1] RSPM (R 4.3.0)\n#&gt;  Rcpp           1.0.12     2024-01-09 [1] RSPM (R 4.3.0)\n#&gt;  RcppParallel   5.1.7      2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;  readxl         1.4.3      2023-07-06 [1] RSPM (R 4.3.0)\n#&gt;  reshape2       1.4.4      2020-04-09 [1] RSPM (R 4.3.0)\n#&gt;  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  rmarkdown      2.25       2023-09-18 [1] RSPM (R 4.3.0)\n#&gt;  rstatix        0.7.2      2023-02-01 [1] RSPM (R 4.3.0)\n#&gt;  scales         1.3.0      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  sessioninfo    1.2.2      2021-12-06 [1] RSPM (R 4.3.0)\n#&gt;  shiny          1.8.0      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  SnowballC      0.7.1      2023-04-25 [1] RSPM (R 4.3.0)\n#&gt;  spam           2.10-0     2023-10-23 [1] RSPM (R 4.3.0)\n#&gt;  stopwords      2.3        2021-10-28 [1] RSPM (R 4.3.0)\n#&gt;  stringi        1.8.3      2024-01-28 [1] https://gagolews.r-universe.dev (R 4.3.2)\n#&gt;  stringr        1.5.1      2023-11-14 [1] RSPM (R 4.3.0)\n#&gt;  styler         1.10.2     2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  systemfonts    1.0.5      2023-10-09 [1] RSPM (R 4.3.0)\n#&gt;  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidygraph      1.3.1      2024-01-30 [1] RSPM (R 4.3.0)\n#&gt;  tidyr          1.3.1      2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidytext       0.4.1      2023-01-07 [1] RSPM (R 4.3.0)\n#&gt;  tokenizers     0.3.0      2022-12-22 [1] RSPM (R 4.3.0)\n#&gt;  utf8           1.2.4      2023-10-22 [1] RSPM (R 4.3.0)\n#&gt;  V8             4.4.2      2024-02-15 [1] CRAN (R 4.3.2)\n#&gt;  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#&gt;  viridis        0.6.5      2024-01-29 [1] RSPM (R 4.3.0)\n#&gt;  viridisLite    0.4.2      2023-05-02 [1] RSPM (R 4.3.0)\n#&gt;  withr          3.0.0      2024-01-16 [1] RSPM (R 4.3.0)\n#&gt;  xfun           0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  xtable         1.8-4      2019-04-21 [1] RSPM (R 4.3.0)\n#&gt;  yaml           2.3.8      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.3\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "docs-menu-1.html",
    "href": "docs-menu-1.html",
    "title": "5  文書メニュー",
    "section": "",
    "text": "5.1 文書検索（A.6.1）",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "docs-menu-1.html#文書検索a.6.1",
    "href": "docs-menu-1.html#文書検索a.6.1",
    "title": "5  文書メニュー",
    "section": "",
    "text": "5.1.1 TF-IDF\nKWICの結果を検索語のTF-IDFの降順で並び替える例。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(section == \"[1]上_先生と私\") |&gt;\n  dplyr::select(label, token) |&gt;\n  dplyr::collect()\n\ndat |&gt;\n  dplyr::reframe(token = list(token), .by = label) |&gt;\n  tibble::deframe() |&gt;\n  quanteda::as.tokens() |&gt;\n  quanteda::tokens_select(\"[[:punct:]]\", selection = \"remove\", valuetype = \"regex\", padding = FALSE) |&gt;\n  quanteda::kwic(pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\") |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::select(docname, pre, keyword, post) |&gt;\n  dplyr::left_join(\n    dat |&gt;\n      dplyr::count(label, token) |&gt;\n      tidytext::bind_tf_idf(token, label, n),\n    by = dplyr::join_by(docname == label, keyword == token)\n  ) |&gt;\n  dplyr::arrange(desc(tf_idf))\n#&gt; # A tibble: 18 × 8\n#&gt;    docname    pre                      keyword post      n      tf   idf  tf_idf\n#&gt;    &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 上・三十四 いっ た なり 下 を       向い    た …      2 0.00251  1.19 0.00298\n#&gt;  2 上・三十四 突然 奥さん の 方 を     向い    た …      2 0.00251  1.19 0.00298\n#&gt;  3 上・八     は 私 の 方 を           向い    て …      2 0.00239  1.19 0.00283\n#&gt;  4 上・八     また 私 の 方 を         向い    た …      2 0.00239  1.19 0.00283\n#&gt;  5 上・二     て 海 の 方 を           向い    て …      2 0.00217  1.19 0.00257\n#&gt;  6 上・二     まで 沖 の 方 へ         向い    て …      2 0.00217  1.19 0.00257\n#&gt;  7 上・十二   な 方角 へ 足 を         向け    た …      1 0.00117  2.20 0.00257\n#&gt;  8 上・十六   の 眼 を 私 に           向け    た …      1 0.00117  2.20 0.00256\n#&gt;  9 上・三十三 先生 は ちょっと 顔 だけ 向け    直し…     1 0.00112  2.20 0.00246\n#&gt; 10 上・二十五 た 世間 に 背中 を       向け    た …      1 0.00107  2.20 0.00236\n#&gt; 11 上・十七   う 世の中 の どっち を   向い    て …      1 0.00122  1.19 0.00145\n#&gt; 12 上・十二   花 より も そちら を     向い    て …      1 0.00117  1.19 0.00139\n#&gt; 13 上・七     は 外 の 方 を           向い    て …      1 0.00116  1.19 0.00138\n#&gt; 14 上・十四   に 庭 の 方 を           向い    た …      1 0.00115  1.19 0.00137\n#&gt; 15 上・三十三 は 庭 の 方 を           向い    て …      1 0.00112  1.19 0.00133\n#&gt; 16 上・三十五 は 庭 の 方 を           向い    て …      1 0.00110  1.19 0.00131\n#&gt; 17 上・十五   は また 奥さん と 差し   向い    で …      1 0.00109  1.19 0.00129\n#&gt; 18 上・二十六 を し て よそ を         向い    て …      1 0.00106  1.19 0.00126\n\n\n\n\n5.1.2 LexRank🍳\nLexRankは、TF-IDFで重みづけした文書間の類似度行列についてページランクを計算することで、文書集合のなかから「重要な文書」を抽出する手法。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    section == \"[3]下_先生と遺書\",\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  )\n\ndfm &lt;- dat |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::bind_tf_idf(token, label, n) |&gt;\n  dplyr::inner_join(\n    dat |&gt;\n      dplyr::select(doc_id, label, token) |&gt;\n      dplyr::collect(),\n    by = dplyr::join_by(label == label, token == token)\n  ) |&gt;\n  tidytext::cast_dfm(doc_id, token, tf_idf)\n\n\n文書間のコサイン類似度を得て、PageRankを計算する。quanteda.textstats::textstat_simil()はproxyC::simil()と処理としては同じだが、戻り値がtextstat_simil_symm_sparseというS4クラスのオブジェクトになっていて、as.data.frame()で縦長のデータフレームに変換できる。\n\n\nCode\nscores &lt;- dfm |&gt;\n  quanteda.textstats::textstat_simil(\n    margin = \"documents\",\n    method = \"cosine\",\n    min_simil = .6 # LexRankの文脈でいうところのthreshold\n  ) |&gt;\n  as.data.frame() |&gt;\n  dplyr::mutate(weight = 1) |&gt; # 閾値以上のエッジしかないので、重みはすべて1にする\n  # dplyr::rename(weight = cosine) |&gt; # あるいは、閾値を指定せずに、コサイン類似度をそのまま重みとして使う（continuous LexRank）\n  igraph::graph_from_data_frame(directed = FALSE) |&gt;\n  igraph::page_rank(directed = FALSE, damping = .85) |&gt;\n  purrr::pluck(\"vector\")\n\n\nLexRankは抽出型の要約アルゴリズムということになっているが、必ずしも要約的な文書が得られるわけではない。文書集合のなかでも類似度が比較的高そうな文書をn件取り出してきてサブセットをつくるみたいな使い方はできるかも？\n\n\nCode\nsort(scores, decreasing = TRUE) |&gt;\n  tibble::enframe() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, text, chapter),\n    by = dplyr::join_by(name == doc_id)\n  ) |&gt;\n  dplyr::slice_head(n = 5)\n#&gt; # A tibble: 5 × 4\n#&gt;   name   value text                                                      chapter\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;                                                     &lt;chr&gt;  \n#&gt; 1 1123  0.0471 「馬鹿だ」とやがてKが答えました。「僕は馬鹿だ」           3_41   \n#&gt; 2 927   0.0353 「とにかくたった一人取り残された私は、母のいい付け通り、… 3_04   \n#&gt; 3 950   0.0323 私はとうとう叔父と談判を開きました。談判というのは少し不… 3_08   \n#&gt; 4 1158  0.0273 私はKがその時何かいいはしなかったかと奥さんに聞きました…  3_47   \n#&gt; 5 1142  0.0228 私は仕方なしに言葉の上で、好い加減にうろつき廻った末、K…  3_44",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "docs-menu-1.html#クラスター分析a.6.2",
    "href": "docs-menu-1.html#クラスター分析a.6.2",
    "title": "5  文書メニュー",
    "section": "5.2 クラスター分析（A.6.2）",
    "text": "5.2 クラスター分析（A.6.2）\n\n5.2.1 LSI🍳\n文書単語行列（または、単語文書行列）に対して特異値分解をおこなって、行列の次元を削減する手法をLSIという。潜在的意味インデキシング（Latent Semantic Indexing, LSI）というのは情報検索の分野での呼び方で、自然言語処理の文脈だと潜在意味解析（Latent Semantic Analysis, LSA）というらしい。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(min_termfreq = 10) |&gt;\n  quanteda::dfm_tfidf(scheme_tf = \"prop\") |&gt;\n  rlang::as_function(\n    ~ quanteda::dfm_subset(., quanteda::rowSums(.) &gt; 0)\n  )()\n\ndfm\n#&gt; Document-feature matrix of: 1,099 documents, 332 features (97.22% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs  書く/動詞  心持/名詞  書生/名詞 出掛ける/動詞 見せる/動詞 始まる/動詞\n#&gt;    1 0.09913172 0.09599561 0             0           0            0        \n#&gt;    2 0          0          0.06058653    0.05825767  0.04353407   0        \n#&gt;    3 0          0          0             0           0            0.1345699\n#&gt;    4 0          0          0             0           0            0        \n#&gt;    5 0          0          0             0.11004226  0            0        \n#&gt;    6 0          0          0             0           0            0        \n#&gt;     features\n#&gt; docs 地位/名詞  来る/動詞   人/名詞C 持つ/動詞\n#&gt;    1 0         0          0.13466723 0        \n#&gt;    2 0         0.05726953 0          0        \n#&gt;    3 0         0          0          0        \n#&gt;    4 0.3827628 0          0          0        \n#&gt;    5 0         0.05408789 0.05237059 0        \n#&gt;    6 0         0          0.06284471 0.0931456\n#&gt; [ reached max_ndoc ... 1,093 more documents, reached max_nfeat ... 322 more features ]\n\n\nここでは300列くらいしかないので大したことないが、特徴量の数が多い文書単語行列をas.matrix()すると、メモリ上でのサイズが大きいオブジェクトになってしまい、扱いづらい。そこで、もとの文書単語行列のもつ情報をできるだけ保持しつつ、行列の次元を削減したいというときに、LSIを利用することができる。\nとくに、文書のクラスタリングをおこなう場合では、どの語彙がどのクラスタに属する要因になっているかみたいなことはどうせ確認できないので、特徴量は適当に削減してしまって問題ないと思う。\n\n\nCode\nlobstr::obj_size(dfm)\n#&gt; 244.62 kB\nlobstr::obj_size(as.matrix(dfm))\n#&gt; 3.02 MB\n\n\nquanteda:textmodels::textmodel_lsa(margin = \"documents\")とすると、特異値分解（Truncated SVD）の \\(D \\simeq D_{k} = U_{k}\\Sigma{}_{k}V^{T}_{k}\\) という式における \\(V_{k}\\) が戻り値にそのまま残る（margin=\"features\"だと \\(U_{k}\\) がそのまま残り、\"both\"で両方ともそのまま残る）。\n特異値分解する行列 \\(D\\) について、いま、行側に文書・列側に単語がある持ち方をしている。ここでは、行列 \\(D\\) をランク \\(k\\) の行列 \\(D_{k}\\) で近似したい（ランク削減したい）というより、特徴量を減らしたい（ \\(k\\) 列の行列にしてしまいたい）と思っているため、dfmに \\(V_{k}\\) をかける。\n\n\nCode\nmat &lt;- quanteda.textmodels::textmodel_lsa(dfm, nd = 50, margin = \"documents\")\nmat &lt;- dfm %*% mat$features\n\nstr(mat)\n#&gt; Formal class 'dgeMatrix' [package \"Matrix\"] with 4 slots\n#&gt;   ..@ Dim     : int [1:2] 1099 50\n#&gt;   ..@ Dimnames:List of 2\n#&gt;   .. ..$ docs: chr [1:1099] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;   .. ..$     : NULL\n#&gt;   ..@ x       : num [1:54950] 0.1055 0.0512 0.0259 0.0294 0.0372 ...\n#&gt;   ..@ factors : list()\n\n\n\n\n5.2.2 階層的クラスタリング\nLSIで次元を削減した行列について、クラスタリングをおこなう。ここでは、文書間の距離としてコサイン距離を使うことにする。\n文書間の距離のイメージ。\n\n\nCode\ng1 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"ejaccard\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"ejaccard\")\n\ng2 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"edice\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"edice\")\n\ng3 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"cosine\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"cosine\")\n\npatchwork::wrap_plots(g1, g2, g3, nrow = 3)\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n階層的クラスタリングは非階層的なアルゴリズムに比べると計算量が多いため、個体数が増えるとクラスタリングするのにやや時間がかかることがある。\n\n\nCode\ndat &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"cosine\") |&gt;\n  rlang::as_function(~ 1 - .)()\n\nclusters &lt;-\n  as.dist(dat) |&gt;\n  hclust(method = \"ward.D2\")\n\ncluster::silhouette(cutree(clusters, k = 5), dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 非階層的クラスタリング🍳\n必ずしもクラスタの階層構造を確認したいわけではない場合、kmeans()だと計算が早いかもしれない。\nただ、「K-meansはクラスタ中心からのユークリッド距離でクラスタを分ける」（機械学習帳）ため、特徴量の数が増えてくるとクラスタの比率がおかしくなりがち。\n\n\nCode\nclusters &lt;- kmeans(mat, centers = 5, iter.max = 100, algorithm = \"Lloyd\")\n\ncluster::silhouette(clusters$cluster, dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\nspherical k-meansの実装であるskmeansだとクラスタの比率はいくらかマシになるかもしれない。\n\n\nCode\nclusters &lt;-\n  skmeans::skmeans(\n    as.matrix(mat),\n    k = 5,\n    method = \"pclust\",\n    control = list(maxiter = 100)\n  )\n\ncluster::silhouette(clusters$cluster, dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "docs-menu-1.html#トピックモデルa.6.3",
    "href": "docs-menu-1.html#トピックモデルa.6.3",
    "title": "5  文書メニュー",
    "section": "5.3 トピックモデル（A.6.3）",
    "text": "5.3 トピックモデル（A.6.3）\n\n5.3.1 トピック数の探索\nLDAのトピック数の探索は、実際にfitしてみて指標のよかったトピック数を採用するみたいなやり方をする。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\n\nここでは、トピック数を5から15まで変化させる。seededlda::textmodel_lda(auto_iter=TRUE)とすると、ステップがmax_iter以下であっても条件によってギブスサンプリングを打ち切る（このWebページのレンダリングにかかる時間を短縮するために指定しています）。\n\n\nCode\ndivergence &lt;-\n  purrr::map(5:15, \\(.x) {\n    lda_fit &lt;-\n      seededlda::textmodel_lda(dfm, k = .x, batch_size = 0.2, auto_iter = TRUE, verbose = FALSE)\n    tibble::tibble(\n      topics = .x,\n      Deveaud2014 = seededlda::divergence(lda_fit, regularize = FALSE),\n      WatanabeBaturo2023 = seededlda::divergence(lda_fit, min_size = .04, regularize = TRUE)\n    )\n  }) |&gt;\n  purrr::list_rbind()\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n#&gt; Warning in x$word: 'word' の 'words' への部分的マッチ\n\n\nDeveaud2014という列は、ldatuningで確認できる同名の値と同じ指標。WatanabeBaturo2023という列は、Deveaud2014についてトピックの比率が閾値を下回るときにペナルティを加えるように修正した指標。どちらも大きいほうがよい指標なので、基本的には値が大きくなっているトピック数を選ぶ。\n\n\nCode\nggplot(divergence, aes(topics)) +\n  geom_line(aes(y = Deveaud2014, color = \"Deveaud2014\")) +\n  geom_line(aes(y = WatanabeBaturo2023, color = \"WatanabeBaturo2023\")) +\n  scale_x_continuous(breaks = 5:15) +\n  theme_bw() +\n  ylab(\"Divergence\")\n#&gt; Warning in vp$just: 'just' の 'justification' への部分的マッチ\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Distributed LDA\n\n\nCode\nlda_fit &lt;-\n  seededlda::textmodel_lda(dfm, k = 9, batch_size = 0.2, verbose = TRUE)\n#&gt; Fitting LDA with 9 topics\n#&gt;  ...initializing\n#&gt;  ...using up to 8 threads for distributed computing\n#&gt;  ......allocating 230 documents to each thread\n#&gt;  ...Gibbs sampling in 2000 iterations\n#&gt;  ......iteration 100 elapsed time: 0.11 seconds (delta: 0.26%)\n#&gt;  ......iteration 200 elapsed time: 0.21 seconds (delta: 0.20%)\n#&gt;  ......iteration 300 elapsed time: 0.30 seconds (delta: -0.42%)\n#&gt;  ......iteration 400 elapsed time: 0.41 seconds (delta: 0.37%)\n#&gt;  ......iteration 500 elapsed time: 0.51 seconds (delta: 0.20%)\n#&gt;  ......iteration 600 elapsed time: 0.61 seconds (delta: -0.53%)\n#&gt;  ......iteration 700 elapsed time: 0.70 seconds (delta: -0.04%)\n#&gt;  ......iteration 800 elapsed time: 0.80 seconds (delta: 0.17%)\n#&gt;  ......iteration 900 elapsed time: 0.90 seconds (delta: -0.05%)\n#&gt;  ......iteration 1000 elapsed time: 1.00 seconds (delta: 0.10%)\n#&gt;  ......iteration 1100 elapsed time: 1.10 seconds (delta: 0.40%)\n#&gt;  ......iteration 1200 elapsed time: 1.20 seconds (delta: 0.02%)\n#&gt;  ......iteration 1300 elapsed time: 1.31 seconds (delta: 0.04%)\n#&gt;  ......iteration 1400 elapsed time: 1.42 seconds (delta: -0.12%)\n#&gt;  ......iteration 1500 elapsed time: 1.54 seconds (delta: -0.22%)\n#&gt;  ......iteration 1600 elapsed time: 1.64 seconds (delta: 0.00%)\n#&gt;  ......iteration 1700 elapsed time: 1.74 seconds (delta: 0.39%)\n#&gt;  ......iteration 1800 elapsed time: 1.84 seconds (delta: 0.31%)\n#&gt;  ......iteration 1900 elapsed time: 1.94 seconds (delta: -0.32%)\n#&gt;  ......iteration 2000 elapsed time: 2.04 seconds (delta: -0.08%)\n#&gt;  ...computing theta and phi\n#&gt;  ...complete\n\nseededlda::sizes(lda_fit)\n#&gt;     topic1     topic2     topic3     topic4     topic5     topic6     topic7 \n#&gt; 0.11043812 0.09903089 0.10645064 0.14455885 0.11599031 0.13754290 0.11215425 \n#&gt;     topic8     topic9 \n#&gt; 0.07914395 0.09469009\n\n\n\n\n5.3.3 トピックとその出現位置\n\n\nCode\ndat &lt;- tbl |&gt;\n  dplyr::transmute(\n    doc_id = doc_id,\n    topic = seededlda::topics(lda_fit)[as.character(doc_id)],\n  ) |&gt;\n  dplyr::filter(!is.na(topic)) # dfmをつくった時点で単語を含まない文書はトピックの割り当てがないため、取り除く\n\ndat |&gt;\n  ggplot(aes(x = doc_id)) +\n  geom_raster(aes(y = topic, fill = topic), show.legend = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n5.3.4 単語の生起確率\n\n\nCode\ndat &lt;-\n  t(lda_fit$phi) |&gt;\n  dplyr::as_tibble(\n    .name_repair = ~ paste0(\"topic\", 1:9),\n    rownames = \"word\"\n  ) |&gt;\n  tidyr::pivot_longer(starts_with(\"topic\"), names_to = \"topic\", values_to = \"phi\") |&gt;\n  dplyr::mutate(phi = signif(phi, 3)) |&gt;\n  dplyr::slice_max(phi, n = 20, by = topic)\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat)\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "docs-menu-1.html#ナイーブベイズ",
    "href": "docs-menu-1.html#ナイーブベイズ",
    "title": "5  文書メニュー",
    "section": "5.4 ナイーブベイズ",
    "text": "5.4 ナイーブベイズ\n\nあとで書く\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package             * version    date (UTC) lib source\n#&gt;  abind                 1.4-5      2016-07-21 [1] RSPM (R 4.3.0)\n#&gt;  audubon               0.5.1.9001 2023-09-03 [1] https://paithiov909.r-universe.dev (R 4.3.1)\n#&gt;  backports             1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#&gt;  blob                  1.2.4      2023-03-17 [1] RSPM (R 4.3.0)\n#&gt;  broom                 1.0.5      2023-06-09 [1] RSPM (R 4.3.0)\n#&gt;  cachem                1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  car                   3.1-2      2023-03-30 [1] RSPM (R 4.3.0)\n#&gt;  carData               3.0-5      2022-01-06 [1] RSPM (R 4.3.0)\n#&gt;  cellranger            1.1.0      2016-07-27 [1] RSPM (R 4.3.0)\n#&gt;  cli                   3.6.2      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  clue                  0.3-65     2023-09-23 [1] RSPM (R 4.3.3)\n#&gt;  cluster               2.1.6      2023-12-01 [4] CRAN (R 4.3.2)\n#&gt;  codetools             0.2-19     2023-02-01 [4] CRAN (R 4.2.2)\n#&gt;  colorspace            2.1-0      2023-01-23 [1] RSPM (R 4.3.0)\n#&gt;  crosstalk             1.2.1      2023-11-23 [1] RSPM (R 4.3.0)\n#&gt;  curl                  5.2.1      2024-03-01 [1] CRAN (R 4.3.3)\n#&gt;  DBI                 * 1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  dbplyr                2.4.0      2023-10-26 [1] RSPM (R 4.3.0)\n#&gt;  digest                0.6.34     2024-01-11 [1] CRAN (R 4.3.2)\n#&gt;  dplyr                 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n#&gt;  duckdb              * 0.9.2-1    2023-11-28 [1] RSPM (R 4.3.2)\n#&gt;  ellipsis              0.3.2      2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;  evaluate              0.23       2023-11-01 [1] CRAN (R 4.3.1)\n#&gt;  factoextra            1.0.7      2020-04-01 [1] RSPM (R 4.3.2)\n#&gt;  fansi                 1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n#&gt;  farver                2.1.1      2022-07-06 [1] RSPM (R 4.3.0)\n#&gt;  fastmap               1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fastmatch             1.1-4      2023-08-18 [1] RSPM (R 4.3.0)\n#&gt;  foreach               1.5.2      2022-02-02 [1] RSPM (R 4.3.0)\n#&gt;  generics              0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2             * 3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  ggpubr                0.6.0      2023-02-10 [1] RSPM (R 4.3.0)\n#&gt;  ggrepel               0.9.5      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  ggsignif              0.6.4      2022-10-13 [1] RSPM (R 4.3.0)\n#&gt;  gibasa                1.1.0      2024-02-17 [1] https://paithiov909.r-universe.dev (R 4.3.2)\n#&gt;  glmnet                4.1-8      2023-08-22 [1] CRAN (R 4.3.1)\n#&gt;  glue                  1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n#&gt;  gtable                0.3.4      2023-08-21 [1] CRAN (R 4.3.1)\n#&gt;  htmltools             0.5.7      2023-11-03 [1] RSPM (R 4.3.0)\n#&gt;  htmlwidgets           1.6.4      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  httpgd                1.3.1      2023-01-30 [1] CRAN (R 4.3.0)\n#&gt;  igraph                2.0.2      2024-02-17 [1] RSPM (R 4.3.0)\n#&gt;  iterators             1.0.14     2022-02-05 [1] RSPM (R 4.3.0)\n#&gt;  janeaustenr           1.0.0      2022-08-26 [1] RSPM (R 4.3.0)\n#&gt;  jsonlite              1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n#&gt;  knitr                 1.45       2023-10-30 [1] CRAN (R 4.3.1)\n#&gt;  labeling              0.4.3      2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  later                 1.3.2      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  lattice               0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  LiblineaR             2.10-23    2023-12-11 [1] RSPM (R 4.3.0)\n#&gt;  lifecycle             1.0.4      2023-11-07 [1] RSPM (R 4.3.0)\n#&gt;  lobstr                1.1.2      2022-06-22 [1] RSPM (R 4.3.0)\n#&gt;  magrittr              2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  Matrix                1.6-3      2023-11-14 [4] CRAN (R 4.3.2)\n#&gt;  memoise               2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  munsell               0.5.0      2018-06-12 [1] RSPM (R 4.3.0)\n#&gt;  nsyllable             1.0.1      2022-02-28 [1] RSPM (R 4.3.0)\n#&gt;  patchwork             1.2.0      2024-01-08 [1] RSPM (R 4.3.0)\n#&gt;  pillar                1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig             2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  plyr                  1.8.9      2023-10-02 [1] RSPM (R 4.3.0)\n#&gt;  prettyunits           1.2.0      2023-09-24 [1] RSPM (R 4.3.0)\n#&gt;  proxyC                0.3.4      2023-10-25 [1] RSPM (R 4.3.0)\n#&gt;  purrr                 1.0.2      2023-08-10 [1] RSPM (R 4.3.0)\n#&gt;  quanteda              3.3.1      2023-05-18 [1] RSPM (R 4.3.0)\n#&gt;  quanteda.textmodels   0.9.6      2023-03-22 [1] RSPM (R 4.3.0)\n#&gt;  quanteda.textstats    0.96.4     2023-11-02 [1] CRAN (R 4.3.1)\n#&gt;  R.cache               0.16.0     2022-07-21 [1] CRAN (R 4.3.0)\n#&gt;  R.methodsS3           1.8.2      2022-06-13 [1] CRAN (R 4.3.0)\n#&gt;  R.oo                  1.26.0     2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  R.utils               2.12.3     2023-11-18 [1] CRAN (R 4.3.2)\n#&gt;  R6                    2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp                  1.0.12     2024-01-09 [1] RSPM (R 4.3.0)\n#&gt;  RcppParallel          5.1.7      2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;  reactable             0.4.4      2023-03-12 [1] RSPM (R 4.3.0)\n#&gt;  reactablefmtr         2.0.0      2022-03-16 [1] RSPM (R 4.3.2)\n#&gt;  reactR                0.5.0      2023-10-11 [1] RSPM (R 4.3.0)\n#&gt;  readxl                1.4.3      2023-07-06 [1] RSPM (R 4.3.0)\n#&gt;  reshape2              1.4.4      2020-04-09 [1] RSPM (R 4.3.0)\n#&gt;  rlang                 1.1.3      2024-01-10 [1] CRAN (R 4.3.2)\n#&gt;  rmarkdown             2.25       2023-09-18 [1] RSPM (R 4.3.0)\n#&gt;  RSpectra              0.16-1     2022-04-24 [1] RSPM (R 4.3.0)\n#&gt;  rstatix               0.7.2      2023-02-01 [1] RSPM (R 4.3.0)\n#&gt;  sass                  0.4.8      2023-12-06 [1] RSPM (R 4.3.0)\n#&gt;  scales                1.3.0      2023-11-28 [1] RSPM (R 4.3.0)\n#&gt;  seededlda             1.1.0      2023-07-01 [1] RSPM (R 4.3.0)\n#&gt;  sessioninfo           1.2.2      2021-12-06 [1] RSPM (R 4.3.0)\n#&gt;  shape                 1.4.6.1    2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  skmeans               0.2-16     2023-09-23 [1] RSPM (R 4.3.3)\n#&gt;  slam                  0.1-50     2022-01-08 [1] RSPM (R 4.3.0)\n#&gt;  SnowballC             0.7.1      2023-04-25 [1] RSPM (R 4.3.0)\n#&gt;  SparseM               1.81       2021-02-18 [1] RSPM (R 4.3.0)\n#&gt;  stopwords             2.3        2021-10-28 [1] RSPM (R 4.3.0)\n#&gt;  stringi               1.8.3      2024-01-28 [1] https://gagolews.r-universe.dev (R 4.3.2)\n#&gt;  stringr               1.5.1      2023-11-14 [1] RSPM (R 4.3.0)\n#&gt;  styler                1.10.2     2023-08-29 [1] RSPM (R 4.3.0)\n#&gt;  survival              3.5-7      2023-08-14 [4] CRAN (R 4.3.1)\n#&gt;  systemfonts           1.0.5      2023-10-09 [1] RSPM (R 4.3.0)\n#&gt;  tibble                3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyr                 1.3.1      2024-01-24 [1] RSPM (R 4.3.0)\n#&gt;  tidyselect            1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidytext              0.4.1      2023-01-07 [1] RSPM (R 4.3.0)\n#&gt;  tokenizers            0.3.0      2022-12-22 [1] RSPM (R 4.3.0)\n#&gt;  utf8                  1.2.4      2023-10-22 [1] RSPM (R 4.3.0)\n#&gt;  V8                    4.4.2      2024-02-15 [1] CRAN (R 4.3.2)\n#&gt;  vctrs                 0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#&gt;  withr                 3.0.0      2024-01-16 [1] RSPM (R 4.3.0)\n#&gt;  xfun                  0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  yaml                  2.3.8      2023-12-11 [1] RSPM (R 4.3.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.3\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Note\n\n\n\n本文の内容に関連する情報をまとめたリストです。いわゆる「参考文献リスト」ではありません\n\n\n\nクラスター分析\n\nk-meansの最適なクラスター数を調べる方法 #Python - Qiita\nシルエット分析 | technical-note\n\nLSI\n\n潜在的意味インデキシング（LSI）徹底入門 - あらびき日記\n\nアソシエーション分析\n\nChapter 5 Association Analysis: Basic Concepts and Algorithms | An R Companion for Introduction to Data Mining\nR でアソシエーション分析 #R - Qiita\n\n共起ネットワーク（ネットワーク分析）\n\nNetwork Analysis Using R\n\n自己組織化マップ\n\naweSOM\n自己組織化マップ入門\n自己組織化マップ - 脳科学辞典",
    "crumbs": [
      "References"
    ]
  }
]