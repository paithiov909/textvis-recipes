[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cookbook to Draw KHCoder-like Visualizations Using R",
    "section": "",
    "text": "Preface\n\nまだ書きかけです……\nちょっとずつ内容が追加されるよ!!\n\n本文についてはCC-BY、掲載されているコードについてはMITライセンスを適用するものとします。\nなんとなく似たような表現をめざしているだけで、KH Coderでおこなわれている処理をRで再実装することをめざすものではありません。"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "1  テキストの前処理",
    "section": "",
    "text": "1.1 使用するデータセット\nKH Coderのチュートリアル用のデータを使う。tutorial_data_3x.zipの中に含まれているtutorial_jp/kokoro.xlsというxlsファイルを次のように読み込んでおく。\nCode\ntbl &lt;-\n  readxl::read_xls(\"tutorial_jp/kokoro.xls\",\n    col_names = c(\"text\", \"section\", \"chapter\", \"label\"),\n    skip = 1\n  ) |&gt;\n  dplyr::mutate(\n    doc_id = dplyr::row_number(),\n    dplyr::across(where(is.character), ~ audubon::strj_normalize(.))\n  ) |&gt;\n  dplyr::filter(!gibasa::is_blank(text)) |&gt;\n  dplyr::relocate(doc_id, text, section, label, chapter)\n\ntbl\n\n\n# A tibble: 1,213 × 5\n   doc_id text                                            section label  chapter\n    &lt;int&gt; &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  \n 1      1 私はその人を常に先生と呼んでいた。だからここで… [1]上_… 上・一 1_01   \n 2      2 私が先生と知り合いになったのは鎌倉である。その… [1]上_… 上・一 1_01   \n 3      3 学校の授業が始まるにはまだ大分日数があるので鎌… [1]上_… 上・一 1_01   \n 4      4 宿は鎌倉でも辺鄙な方角にあった。玉突きだのアイ… [1]上_… 上・一 1_01   \n 5      5 私は毎日海へはいりに出掛けた。古い燻ぶり返った… [1]上_… 上・一 1_01   \n 6      6 私は実に先生をこの雑沓の間に見付け出したのであ… [1]上_… 上・一 1_01   \n 7      7 私がその掛茶屋で先生を見た時は、先生がちょうど… [1]上_… 上・二 1_02   \n 8      8 その西洋人の優れて白い皮膚の色が、掛茶屋へ入る… [1]上_… 上・二 1_02   \n 9      9 彼はやがて自分の傍を顧みて、そこにこごんでいる… [1]上_… 上・二 1_02   \n10     10 私は単に好奇心のために、並んで浜辺を下りて行く… [1]上_… 上・二 1_02   \n# ℹ 1,203 more rows\nこのデータでは、夏目漱石の『こころ』が段落（doc_id）ごとにひとつのテキストとして打ち込まれている。『こころ』は上中下の3部（section）で構成されていて、それぞれの部が複数の章（label, chapter）に分かれている。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>テキストの前処理</span>"
    ]
  },
  {
    "objectID": "preprocessing.html#語の抽出a.2.2",
    "href": "preprocessing.html#語の抽出a.2.2",
    "title": "1  テキストの前処理",
    "section": "1.2 語の抽出（A.2.2）",
    "text": "1.2 語の抽出（A.2.2）\ngibasaを使って形態素解析をおこない、語を抽出する。\nこのデータをIPA辞書を使って形態素解析すると、延べ語数は105,000語程度になる。これくらいの語数であれば、形態素解析した結果をデータフレームとしてメモリ上に読み込んでも問題ないと思われるが、ここではより大規模なテキストデータを扱う場合を想定し、結果をDuckDBデータベースに書き込むことにする。\nここではchapterごとにグルーピングしながら、段落は文に分割せずに処理している。MeCabはバッファサイズの都合上、一度に262万字くらいまで一つの文として入力できるらしいが、極端に長い入力に対してはコスト計算ができず、エラーが出る可能性がある。また、多くの文を与えればそれだけ多くの行からなるデータフレームが返されるため、一度に処理する分量は利用している環境にあわせて適当に加減したほうがよい。\nKH Coderでは、IPA辞書の品詞体系をもとに変更した品詞体系が使われている。そのため、KH Coderで前処理した結果をある程度再現するためには、一部の品詞情報を書き換える必要がある。KH Coder内で使われている品詞体系については、KH Coderのレファレンスを参照されたい。\nまた、このデータを使っているチュートリアルでは、強制抽出する語として「一人」「二人」という語を指定している。こうした語についてはMeCabのユーザー辞書に追加してしまったほうがよいが、簡単に処理するために、ここではgibasaの制約付き解析機能によって「タグ」として抽出している（KH Coderは強制抽出した語に対して「タグ」という品詞名を与える）。\n\n\nCode\nsuppressPackageStartupMessages({\n  library(duckdb)\n})\ndrv &lt;- duckdb::duckdb()\n\nif (!fs::file_exists(\"tutorial_jp/kokoro.duckdb\")) {\n\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = FALSE)\n\n  dbCreateTable(\n    con, \"tokens\",\n    data.frame(\n      doc_id = integer(),\n      section = character(),\n      label = character(),\n      token_id = integer(),\n      token = character(),\n      pos = character(),\n      original = character(),\n      stringsAsFactors = FALSE\n    )\n  )\n\n  tbl |&gt;\n    dplyr::group_by(chapter) |&gt;\n    dplyr::group_walk(~ {\n      df &lt;- .x |&gt;\n        dplyr::mutate(\n          text = stringi::stri_replace_all_regex(text, \"(?&lt;codes&gt;([一二三四五六七八九]{1}人))\", \"\\n${codes}\\tタグ\\n\") |&gt;\n            stringi::stri_trim_both()\n        ) |&gt;\n        gibasa::tokenize(text, doc_id, partial = TRUE) |&gt;\n        gibasa::prettify(\n          col_select = c(\"POS1\", \"POS2\", \"POS3\", \"Original\")\n        ) |&gt;\n        dplyr::mutate(\n          pos = dplyr::case_when(\n            (POS1 == \"タグ\") ~ \"タグ\",\n            (is.na(Original) & stringr::str_detect(token, \"^[[:alpha:]]+$\")) ~ \"未知語\",\n            (POS1 == \"感動詞\") ~ \"感動詞\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Han}]{1}$\")) ~ \"名詞C\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"名詞B\",\n            (POS1 == \"名詞\" & POS2 == \"一般\") ~ \"名詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"地域\") ~ \"地名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"人名\") ~ \"人名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"組織\") ~ \"組織名\",\n            (POS1 == \"名詞\" & POS2 == \"形容動詞語幹\") ~ \"形容動詞\",\n            (POS1 == \"名詞\" & POS2 == \"ナイ形容詞語幹\") ~ \"ナイ形容詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\") ~ \"固有名詞\",\n            (POS1 == \"名詞\" & POS2 == \"サ変接続\") ~ \"サ変名詞\",\n            (POS1 == \"名詞\" & POS2 == \"副詞可能\") ~ \"副詞可能\",\n            (POS1 == \"動詞\" & POS2 == \"自立\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"動詞B\",\n            (POS1 == \"動詞\" & POS2 == \"自立\") ~ \"動詞\",\n            (POS1 == \"形容詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"形容詞B\",\n            (POS1 == \"形容詞\" & POS2 == \"非自立\") ~ \"形容詞（非自立）\",\n            (POS1 == \"形容詞\") ~ \"形容詞\",\n            (POS1 == \"副詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"副詞B\",\n            (POS1 == \"副詞\") ~ \"副詞\",\n            (POS1 == \"助動詞\" & Original %in% c(\"ない\", \"まい\", \"ぬ\", \"ん\")) ~ \"否定助動詞\",\n            .default = \"その他\"\n          )\n        ) |&gt;\n        dplyr::select(doc_id, section, label, token_id, token, pos, Original) |&gt;\n        dplyr::rename(original = Original)\n\n      dbAppendTable(con, \"tokens\", df)\n    })\n} else {\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = TRUE)\n}"
  },
  {
    "objectID": "preprocessing.html#コーディングルールa.2.5",
    "href": "preprocessing.html#コーディングルールa.2.5",
    "title": "1  テキストの前処理",
    "section": "1.3 コーディングルール（A.2.5）",
    "text": "1.3 コーディングルール（A.2.5）\nKH Coderの強力な機能のひとつとして、「コーディングルール」によるトークンへのタグ付けというのがある。KH Coderのコーディングルールはかなり複雑な記法を扱うため、Rで完璧に再現するには相応の手間がかかる。一方で、コードを与えるべき抽出語を基本形とマッチングする程度であれば、次のように比較的少ないコード量で似たようなことを実現できる。\n\n\nCode\nrules &lt;- list(\n  \"人の死\" = c(\"死後\", \"死病\", \"死期\", \"死因\", \"死骸\", \"生死\", \"自殺\", \"殉死\", \"頓死\", \"変死\", \"亡\", \"死ぬ\", \"亡くなる\", \"殺す\", \"亡くす\", \"死\"),\n  \"恋愛\" = c(\"愛\", \"恋\", \"愛す\", \"愛情\", \"恋人\", \"愛人\", \"恋愛\", \"失恋\", \"恋しい\"),\n  \"友情\" = c(\"友達\", \"友人\", \"旧友\", \"親友\", \"朋友\", \"友\", \"級友\"),\n  \"信用・不信\" = c(\"信用\", \"信じる\", \"信ずる\", \"不信\", \"疑い\", \"疑惑\", \"疑念\", \"猜疑\", \"狐疑\", \"疑問\", \"疑い深い\", \"疑う\", \"疑る\", \"警戒\"),\n  \"病気\" = c(\"医者\", \"病人\", \"病室\", \"病院\", \"病症\", \"病状\", \"持病\", \"死病\", \"主治医\", \"精神病\", \"仮病\", \"病気\", \"看病\", \"大病\", \"病む\", \"病\")\n)\n\ncodes &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(original %in% purrr::flatten_chr(rules)) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::mutate(\n    codings = purrr::map(original,\n      ~ purrr::imap(rules, \\(.x, .y) tibble::tibble(code = .y, flag = . %in% .x)) |&gt;\n      purrr::list_rbind() |&gt;\n      dplyr::filter(flag == TRUE) |&gt;\n      dplyr::select(!flag)\n    )\n  ) |&gt;\n  tidyr::unnest(codings)\n\ncodes\n\n\n# A tibble: 537 × 8\n   doc_id section        label  token_id token    pos      original code      \n    &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     \n 1      2 [1]上_先生と私 上・一       36 友達     名詞     友達     友情      \n 2      2 [1]上_先生と私 上・一       96 友達     名詞     友達     友情      \n 3      2 [1]上_先生と私 上・一      115 病気     サ変名詞 病気     病気      \n 4      2 [1]上_先生と私 上・一      124 友達     名詞     友達     友情      \n 5      2 [1]上_先生と私 上・一      128 信じ     動詞     信じる   信用・不信\n 6      2 [1]上_先生と私 上・一      132 友達     名詞     友達     友情      \n 7      2 [1]上_先生と私 上・一      240 病気     サ変名詞 病気     病気      \n 8      3 [1]上_先生と私 上・一       44 友達     名詞     友達     友情      \n 9     19 [1]上_先生と私 上・三      207 疑っ     動詞     疑う     信用・不信\n10     21 [1]上_先生と私 上・四      161 亡くなっ 動詞     亡くなる 人の死    \n# ℹ 527 more rows"
  },
  {
    "objectID": "preprocessing.html#抽出語リストa.3.4",
    "href": "preprocessing.html#抽出語リストa.3.4",
    "title": "1  テキストの前処理",
    "section": "1.4 抽出語リスト（A.3.4）",
    "text": "1.4 抽出語リスト（A.3.4）\n「エクスポート」メニューから得られるような抽出語リストをデータフレームとして得る例。\nExcel向けの出力は見やすいようにカラムを分けているが、Rのデータフレームとして扱うならtidyな縦長のデータにしたほうがよい。\n\n1.4.1 品詞別・上位15語\n\n\nCode\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 15, by = pos) |&gt;\n  dplyr::collect()\n\n\n# A tibble: 232 × 3\n   token  pos          n\n   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n 1 前     副詞可能   169\n 2 今     副詞可能   140\n 3 場合   副詞可能    38\n 4 過去   副詞可能    34\n 5 結果   副詞可能    30\n 6 すべて 副詞可能    28\n 7 平生   副詞可能    26\n 8 時間   副詞可能    25\n 9 今度   副詞可能    24\n10 事実   副詞可能    22\n# ℹ 222 more rows\n\n\n\n\n1.4.2 頻出150語\n\n\nCode\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !is.na(original),\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 150) |&gt;\n  dplyr::collect()\n\n\n# A tibble: 152 × 3\n   token  pos       n\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 先生   名詞    595\n 2 奥さん 名詞    388\n 3 思う   動詞    293\n 4 父     名詞C   269\n 5 自分   名詞    264\n 6 見る   動詞    225\n 7 聞く   動詞    218\n 8 出る   動詞    179\n 9 人     名詞C   176\n10 母     名詞C   170\n# ℹ 142 more rows"
  },
  {
    "objectID": "preprocessing.html#文書抽出語表a.3.5",
    "href": "preprocessing.html#文書抽出語表a.3.5",
    "title": "1  テキストの前処理",
    "section": "1.5 「文書・抽出語」表（A.3.5）",
    "text": "1.5 「文書・抽出語」表（A.3.5）\nいわゆる文書単語行列の例。dplyr::collectした後にtidyr::pivot_wider()などで横に展開してもよいが、多くの場合、疎行列のオブジェクトにしてしまったほうが、この後にRでの解析に用いる上では扱いやすいと思われる。quantedaのdfmオブジェクトをふつうの密な行列にしたいときは、as.matrix(dfm)すればよい。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !is.na(original),\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(min_termfreq = 75, termfreq_type = \"rank\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n\n\nDocument-feature matrix of: 1,185 documents, 78 features (93.19% sparse) and 1 docvar.\n    features\ndocs 先生/名詞 取る/動詞 思う/動詞 女/名詞C 知れる/動詞 立つ/動詞 来る/動詞\n   1         3         0         0        0           0         0         0\n   2         1         0         0        0           0         0         2\n   3         0         0         0        0           0         0         0\n   4         0         1         0        0           0         0         0\n   5         0         0         1        1           0         0         1\n   6         1         0         0        0           0         0         0\n    features\ndocs 見る/動詞 置く/動詞 頭/名詞C\n   1         0         0        0\n   2         0         0        0\n   3         0         0        0\n   4         0         0        0\n   5         0         0        1\n   6         0         0        0\n[ reached max_ndoc ... 1,179 more documents, reached max_nfeat ... 68 more features ]"
  },
  {
    "objectID": "preprocessing.html#文書コード表a.3.6",
    "href": "preprocessing.html#文書コード表a.3.6",
    "title": "1  テキストの前処理",
    "section": "1.6 「文書・コード」表（A.3.6）",
    "text": "1.6 「文書・コード」表（A.3.6）\n「文書・コード」行列の例。コードの出現頻度ではなく「コードの有無をあらわす2値変数」を出力する。\n\n\nCode\ndfm &lt;- codes |&gt;\n  dplyr::count(doc_id, code) |&gt;\n  tidytext::cast_dfm(doc_id, code, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n\n\nDocument-feature matrix of: 294 documents, 5 features (76.19% sparse) and 1 docvar.\n    features\ndocs 信用・不信 友情 病気 人の死 恋愛\n  2           1    1    1      0    0\n  3           0    1    0      0    0\n  19          1    0    0      0    0\n  21          0    0    0      1    0\n  37          0    0    0      1    0\n  49          0    1    0      0    0\n[ reached max_ndoc ... 288 more documents ]\n\n\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)"
  },
  {
    "objectID": "plot-words.html",
    "href": "plot-words.html",
    "title": "2  抽出語メニュー",
    "section": "",
    "text": "2.1 抽出語リスト（A.5.1）\n活用語をクリックするとそれぞれの活用の出現頻度も見れるUIは、Rだとどうすれば実現できるかわからない。\nCode\ndat &lt;- dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::filter(n &gt;= 100) |&gt;\n  dplyr::collect()\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat)\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>抽出語メニュー</span>"
    ]
  },
  {
    "objectID": "plot-words.html#出現回数の分布a.5.2",
    "href": "plot-words.html#出現回数の分布a.5.2",
    "title": "2  抽出語メニュー",
    "section": "2.2 出現回数の分布（A.5.2）",
    "text": "2.2 出現回数の分布（A.5.2）\n\n2.2.1 度数分布表\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::summarise(\n    degree = sum(n, na.rm = TRUE),\n    .by = n\n  ) |&gt;\n  dplyr::mutate(\n    prop = degree / sum(degree, na.rm = TRUE)\n  ) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::compute() |&gt;\n  dplyr::mutate(\n    cum_degree = cumsum(degree),\n    cum_prop = cumsum(prop)\n  ) |&gt;\n  dplyr::collect()\n\ndat\n\n\n# A tibble: 98 × 5\n       n degree   prop cum_degree cum_prop\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   2938 0.111        2938    0.111\n 2     2   1934 0.0729       4872    0.184\n 3     3   1506 0.0568       6378    0.240\n 4     4   1248 0.0470       7626    0.287\n 5     5    955 0.0360       8581    0.323\n 6     6    768 0.0289       9349    0.352\n 7     7    868 0.0327      10217    0.385\n 8     8    592 0.0223      10809    0.407\n 9     9    531 0.0200      11340    0.427\n10    10    570 0.0215      11910    0.449\n# ℹ 88 more rows\n\n\n\n\n2.2.2 折れ線グラフ\n\n\nCode\ndat |&gt;\n  dplyr::filter(cum_prop &lt; .8) |&gt;\n  ggplot(aes(x = n, y = degree)) +\n  geom_line() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"度数\")"
  },
  {
    "objectID": "plot-words.html#文書数の分布a.5.3",
    "href": "plot-words.html#文書数の分布a.5.3",
    "title": "2  抽出語メニュー",
    "section": "2.3 文書数の分布（A.5.3）",
    "text": "2.3 文書数の分布（A.5.3）\n段落（doc_id）ではなく、章ラベル（label）でグルーピングして集計している。docfreqについて上でやったのと同様に処理すれば度数分布表にできる。\n\n\nCode\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = paste(token, pos, sep = \"/\")) |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(label, token, n) |&gt;\n  quanteda.textstats::textstat_frequency() |&gt;\n  dplyr::as_tibble()\n\ndat\n\n\n# A tibble: 5,797 × 5\n   feature       frequency  rank docfreq group\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 先生/名詞           595     1      53 all  \n 2 K/未知語            411     2      36 all  \n 3 奥さん/名詞         388     3      54 all  \n 4 父/名詞C            269     4      41 all  \n 5 自分/名詞           264     5      93 all  \n 6 人/名詞C            176     6      84 all  \n 7 母/名詞C            170     7      38 all  \n 8 前/副詞可能         169     8      97 all  \n 9 お嬢さん/名詞       168     9      34 all  \n10 見/動詞             157    10      78 all  \n# ℹ 5,787 more rows\n\n\n度数分布をグラフで確認したいだけなら、このかたちからヒストグラムを描いたほうが楽。\n\n\nCode\ndat |&gt;\n  ggplot(aes(x = docfreq)) +\n  geom_histogram(binwidth = 3) +\n  scale_y_sqrt() +\n  theme_bw() +\n  labs(x = \"文書数\", y = \"度数\")"
  },
  {
    "objectID": "plot-words.html#出現回数文書数のプロットa.5.4",
    "href": "plot-words.html#出現回数文書数のプロットa.5.4",
    "title": "2  抽出語メニュー",
    "section": "2.4 出現回数・文書数のプロット（A.5.4）",
    "text": "2.4 出現回数・文書数のプロット（A.5.4）\n図 A.25のようなグラフの例。ggplot2でgraphics::identifyのようなことをするやり方がわからないので、適当に条件を指定してgghighlightでハイライトしている。\n\n\nCode\ndat |&gt;\n  ggplot(aes(x = frequency, y = docfreq)) +\n  geom_jitter() +\n  gghighlight::gghighlight(\n    frequency &gt; 100 & docfreq &lt; 60\n  ) +\n  ggrepel::geom_text_repel(aes(label = feature)) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"文書数\")"
  },
  {
    "objectID": "plot-words.html#kwica.5.5",
    "href": "plot-words.html#kwica.5.5",
    "title": "2  抽出語メニュー",
    "section": "2.5 KWIC（A.5.5）",
    "text": "2.5 KWIC（A.5.5）"
  },
  {
    "objectID": "plot-words.html#関連語検索a.5.6",
    "href": "plot-words.html#関連語検索a.5.6",
    "title": "2  抽出語メニュー",
    "section": "2.6 関連語検索（A.5.6）",
    "text": "2.6 関連語検索（A.5.6）"
  },
  {
    "objectID": "plot-words.html#対応分析a.5.7",
    "href": "plot-words.html#対応分析a.5.7",
    "title": "2  抽出語メニュー",
    "section": "2.7 対応分析（A.5.7）",
    "text": "2.7 対応分析（A.5.7）\n\n2.7.1 バイプロット\n段落（doc_id）内の頻度で語彙を削ってから部（section）ごとに集計するために、ややめんどうなことをしている。\n\n\nCode\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(\n    min_termfreq = 75,\n    termfreq_type = \"rank\",\n    min_docfreq = 20,\n    docfreq_type = \"count\"\n  )\n\n\nこうしてdoc_idごとに集計したdfmオブジェクトを一度tidytext::tidy()して3つ組のデータフレームに戻し、sectionのラベルを結合する。このデータフレームをもう一度tidytext::cast_dfm()で疎行列に変換して、quanteda.textmodels::textmodel_ca()を使って対応分析にかける。\nこの関数は疎行列に対して計算をおこなえるため、比較的大きな行列を渡しても大丈夫。\n\n\nCode\nlibrary(ca)\n\nca_fit &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  tidytext::cast_dfm(section, term, count) |&gt;\n  quanteda.textmodels::textmodel_ca(nd = 2, sparse = TRUE)\n\ndat &lt;- plot(ca_fit)\n\n\n\n\n\ncaパッケージを読み込んでいるとplot()でバイプロットを描ける。factoextra::fviz_ca_biplot()でも描けるが、見た目はplot()のとあまり変わらない。\n\n\n2.7.2 バイプロット（バブルプロット）\nggplot2でバイプロットを描画するには、たとえば次のようにする。ggrepel::geom_text_repelでラベルを出す語彙の選択の仕方はもうすこし工夫したほうがよいかもしれない。\nなお、このコードはCorrespondence Analysis visualization using ggplot | R-bloggersを参考にした。\n\n\nCode\ntf &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  dplyr::summarise(tf = sum(count), .by = term) |&gt;\n  dplyr::pull(tf, term)\n\n# modified from https://www.r-bloggers.com/2019/08/correspondence-analysis-visualization-using-ggplot/\nmake_ca_plot_df &lt;- function(ca.plot.obj, row.lab = \"Rows\", col.lab = \"Columns\") {\n  tibble::tibble(\n    Label = c(\n      rownames(ca.plot.obj$rows),\n      rownames(ca.plot.obj$cols)\n    ),\n    Dim1 = c(\n      ca.plot.obj$rows[, 1],\n      ca.plot.obj$cols[, 1]\n    ),\n    Dim2 = c(\n      ca.plot.obj$rows[, 2],\n      ca.plot.obj$cols[, 2]\n    ),\n    Variable = c(\n      rep(row.lab, nrow(ca.plot.obj$rows)),\n      rep(col.lab, nrow(ca.plot.obj$cols))\n    )\n  )\n}\ndat &lt;- dat |&gt;\n  make_ca_plot_df(row.lab = \"Construction\", col.lab = \"Medium\") |&gt;\n  dplyr::mutate(\n    Size = dplyr::if_else(Variable == \"Construction\", mean(tf), tf[Label])\n  )\n# 非ASCII文字のラベルに対してwarningを出さないようにする\nsuppressWarnings({\n  ca_sum &lt;- summary(ca_fit)\n  dim_var_percs &lt;- ca_sum$scree[, \"values2\"]\n})\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, col = Variable, label = Label)) +\n  geom_vline(xintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_hline(yintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_jitter(aes(size = Size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_label_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Construction\"),\n    show.legend = FALSE\n  ) +\n  ggrepel::geom_text_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Medium\", sqrt(Dim1^2 + Dim2^2) &gt; 0.25),\n    show.legend = FALSE\n  ) +\n  scale_x_continuous(\n    limits = range(dat$Dim1) +\n      c(diff(range(dat$Dim1)) * -0.2, diff(range(dat$Dim1)) * 0.2)) +\n  scale_y_continuous(\n    limits = range(dat$Dim2) +\n      c(diff(range(dat$Dim2)) * -0.2, diff(range(dat$Dim2)) * 0.2)) +\n  scale_size_area(max_size = 16) +\n  labs(\n    x = paste0(\"Dimension 1 (\", signif(dim_var_percs[1], 3), \"%)\"),\n    y = paste0(\"Dimension 2 (\", signif(dim_var_percs[2], 3), \"%)\")\n  ) +\n  theme_classic()\n\n\nWarning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "plot-words.html#多次元尺度構成法a.5.8",
    "href": "plot-words.html#多次元尺度構成法a.5.8",
    "title": "2  抽出語メニュー",
    "section": "2.8 多次元尺度構成法（A.5.8）",
    "text": "2.8 多次元尺度構成法（A.5.8）\n\n2.8.1 2次元・バブルプロット\nMASS::isoMDSよりMASS::sammonのほうがたぶん見やすい。\n\n\nCode\nsimil &lt;- dfm |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\")\n\ndat &lt;- MASS::sammon(1 - simil, k = 2) |&gt;\n  purrr::pluck(\"points\")\n\n\nInitial stress        : 0.62279\nstress after   0 iters: 0.62279\n\n\n\n\nCode\ndat &lt;- dat |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"label\") |&gt;\n  dplyr::rename(Dim1 = V1, Dim2 = V2) |&gt;\n  dplyr::mutate(\n    size = tf[label],\n    clust = (hclust(\n      proxyC::dist(dat, method = \"euclidean\") |&gt; as.dist(),\n      method = \"ward.D2\"\n    ) |&gt; cutree(k = 8))[label]\n  )\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, label = label, col = factor(clust))) +\n  geom_point(aes(size = size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_text_repel(show.legend = FALSE) +\n  scale_size_area(max_size = 16) +\n  theme_classic()\n\n\nWarning: ggrepel: 23 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n2.8.2 3次元プロット\nrglではなくplotlyで試してみたが、とくに見やすいということはなかったかもしれない。\n\n\nCode\ndat &lt;- MASS::sammon(1 - simil, k = 3) |&gt;\n  purrr::pluck(\"points\") |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"label\") |&gt;\n  dplyr::rename(Dim1 = V1, Dim2 = V2, Dim3 = V3) |&gt;\n  dplyr::mutate(term_freq = tf[label])\n\n\nInitial stress        : 0.53925\nstress after  10 iters: 0.18929, magic = 0.092\nstress after  20 iters: 0.14417, magic = 0.043\nstress after  21 iters: 0.13794\n\n\n\n\nCode\ndat |&gt;\n  plotly::plot_ly(x = ~Dim1, y = ~Dim2, z = ~Dim3, text = ~label, color = ~term_freq) |&gt;\n  plotly::add_markers(opacity = .5)"
  },
  {
    "objectID": "plot-words.html#階層的クラスタリング",
    "href": "plot-words.html#階層的クラスタリング",
    "title": "2  抽出語メニュー",
    "section": "2.9 階層的クラスタリング",
    "text": "2.9 階層的クラスタリング\n\nデンドログラムのaxisの外に棒グラフを描く処理はこのへんと思われる\n透過したグラフを絶対位置で重ねるみたいな処理のよう"
  },
  {
    "objectID": "plot-words.html#共起ネットワーク",
    "href": "plot-words.html#共起ネットワーク",
    "title": "2  抽出語メニュー",
    "section": "2.10 共起ネットワーク",
    "text": "2.10 共起ネットワーク"
  },
  {
    "objectID": "plot-words.html#自己組織化マップsom",
    "href": "plot-words.html#自己組織化マップsom",
    "title": "2  抽出語メニュー",
    "section": "2.11 自己組織化マップ（SOM）",
    "text": "2.11 自己組織化マップ（SOM）\n\n\n\nCode\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)"
  },
  {
    "objectID": "preprocessing.html#使用するデータセット",
    "href": "preprocessing.html#使用するデータセット",
    "title": "1  テキストの前処理",
    "section": "1.1 使用するデータセット",
    "text": "1.1 使用するデータセット\nKH Coderのチュートリアル用のデータを使う。tutorial_data_3x.zipの中に含まれているtutorial_jp/kokoro.xlsというxlsファイルを次のように読み込んでおく。\n\n\nCode\ntbl &lt;-\n  readxl::read_xls(\"tutorial_jp/kokoro.xls\",\n    col_names = c(\"text\", \"section\", \"chapter\", \"label\"),\n    skip = 1\n  ) |&gt;\n  dplyr::mutate(\n    doc_id = dplyr::row_number(),\n    dplyr::across(where(is.character), ~ audubon::strj_normalize(.))\n  ) |&gt;\n  dplyr::filter(!gibasa::is_blank(text)) |&gt;\n  dplyr::relocate(doc_id, text, section, label, chapter)\n\ntbl\n\n\n# A tibble: 1,213 × 5\n   doc_id text                                            section label  chapter\n    &lt;int&gt; &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  \n 1      1 私はその人を常に先生と呼んでいた。だからここで… [1]上_… 上・一 1_01   \n 2      2 私が先生と知り合いになったのは鎌倉である。その… [1]上_… 上・一 1_01   \n 3      3 学校の授業が始まるにはまだ大分日数があるので鎌… [1]上_… 上・一 1_01   \n 4      4 宿は鎌倉でも辺鄙な方角にあった。玉突きだのアイ… [1]上_… 上・一 1_01   \n 5      5 私は毎日海へはいりに出掛けた。古い燻ぶり返った… [1]上_… 上・一 1_01   \n 6      6 私は実に先生をこの雑沓の間に見付け出したのであ… [1]上_… 上・一 1_01   \n 7      7 私がその掛茶屋で先生を見た時は、先生がちょうど… [1]上_… 上・二 1_02   \n 8      8 その西洋人の優れて白い皮膚の色が、掛茶屋へ入る… [1]上_… 上・二 1_02   \n 9      9 彼はやがて自分の傍を顧みて、そこにこごんでいる… [1]上_… 上・二 1_02   \n10     10 私は単に好奇心のために、並んで浜辺を下りて行く… [1]上_… 上・二 1_02   \n# ℹ 1,203 more rows\n\n\nこのデータでは、夏目漱石の『こころ』が段落（doc_id）ごとにひとつのテキストとして打ち込まれている。『こころ』は上中下の3部（section）で構成されていて、それぞれの部が複数の章（label, chapter）に分かれている。"
  },
  {
    "objectID": "plot-words.html#抽出語リストa.5.1",
    "href": "plot-words.html#抽出語リストa.5.1",
    "title": "2  抽出語メニュー",
    "section": "2.1 抽出語リスト（A.5.1）",
    "text": "2.1 抽出語リスト（A.5.1）\n活用語をクリックするとそれぞれの活用の出現頻度も見れるUIは、Rだとどうすれば実現できるかわからない。\n\n\nCode\ndat &lt;- dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::filter(n &gt;= 100) |&gt;\n  dplyr::collect()\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat)\n  )\n)"
  }
]