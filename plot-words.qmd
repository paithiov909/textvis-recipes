# 抽出語メニュー

```{r}
#| label: setup
suppressPackageStartupMessages({
  library(ggplot2)
  library(duckdb)
  library(arules)
  library(arulesViz)
  library(ca)
})
drv <- duckdb::duckdb()
con <- duckdb::dbConnect(drv, dbdir = "tutorial_jp/kokoro.duckdb", read_only = TRUE)

tbl <-
  readxl::read_xls("tutorial_jp/kokoro.xls",
    col_names = c("text", "section", "chapter", "label"),
    skip = 1
  ) |>
  dplyr::mutate(
    doc_id = factor(dplyr::row_number()),
    dplyr::across(where(is.character), ~ audubon::strj_normalize(.))
  ) |>
  dplyr::filter(!gibasa::is_blank(text)) |>
  dplyr::relocate(doc_id, text, section, label, chapter)
```

---

## 抽出語リスト（A.5.1）

活用語をクリックするとそれぞれの活用の出現頻度も見れるUIについては、Rだとどうすれば実現できるかわからない。

```{r}
#| label: freq-list
dat <- dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    !pos %in% c("その他", "名詞B", "動詞B", "形容詞B", "副詞B", "否定助動詞", "形容詞（非自立）")
  ) |>
  dplyr::count(token, pos) |>
  dplyr::filter(n >= 100) |>
  dplyr::collect()

reactable::reactable(
  dat,
  filterable = TRUE,
  defaultColDef = reactable::colDef(
    cell = reactablefmtr::data_bars(dat)
  )
)
```

## 出現回数の分布（A.5.2）

### 度数分布表

```{r}
#| label: calc-tf
dat <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    !pos %in% c("その他", "名詞B", "動詞B", "形容詞B", "副詞B", "否定助動詞", "形容詞（非自立）")
  ) |>
  dplyr::count(token, pos) |>
  dplyr::summarise(
    degree = sum(n, na.rm = TRUE),
    .by = n
  ) |>
  dplyr::mutate(
    prop = degree / sum(degree, na.rm = TRUE)
  ) |>
  dplyr::arrange(n) |>
  dplyr::compute() |>
  dplyr::mutate(
    cum_degree = cumsum(degree),
    cum_prop = cumsum(prop)
  ) |>
  dplyr::collect()

dat
```

### 折れ線グラフ

```{r}
#| label: plot-tf-dist
dat |>
  dplyr::filter(cum_prop < .8) |>
  ggplot(aes(x = n, y = degree)) +
  geom_line() +
  theme_bw() +
  labs(x = "出現回数", y = "度数")
```

## 文書数の分布（A.5.3）

段落（`doc_id`）ではなく、章ラベル（`label`）でグルーピングして集計している。`docfreq`について上でやったのと同様に処理すれば度数分布表にできる。

```{r}
#| label: calc-df
dat <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    !pos %in% c("その他", "名詞B", "動詞B", "形容詞B", "副詞B", "否定助動詞", "形容詞（非自立）")
  ) |>
  dplyr::mutate(token = paste(token, pos, sep = "/")) |>
  dplyr::count(label, token) |>
  dplyr::collect() |>
  tidytext::cast_dfm(label, token, n) |>
  quanteda.textstats::textstat_frequency() |>
  dplyr::as_tibble()

dat
```

度数分布をグラフで確認したいだけなら、このかたちからヒストグラムを描いたほうが楽。

```{r}
#| label: plot-df-hist
dat |>
  ggplot(aes(x = docfreq)) +
  geom_histogram(binwidth = 3) +
  scale_y_sqrt() +
  theme_bw() +
  labs(x = "文書数", y = "度数")
```

## 出現回数・文書数のプロット（A.5.4）

図 A.25のようなグラフの例。ggplot2で`graphics::identify`のようなことをするやり方がわからないので、適当に条件を指定してgghighlightでハイライトしている。

```{r}
#| label: plot-tf-df
#| cache: true
dat |>
  ggplot(aes(x = frequency, y = docfreq)) +
  geom_jitter() +
  gghighlight::gghighlight(
    frequency > 100 & docfreq < 60
  ) +
  ggrepel::geom_text_repel(aes(label = feature)) +
  scale_x_log10() +
  theme_bw() +
  labs(x = "出現回数", y = "文書数")
```

## KWIC（A.5.5）

### コンコーダンス

コンコーダンスは`quanteda::kwic()`で確認できる。もっとも、KH Coderの提供するコンコーダンス検索やコロケーション統計のほうが明らかにリッチなのと、Rのコンソールは日本語の長めの文字列を表示するのにあまり向いていないというのがあるので、このあたりの機能が必要ならKH Coderを利用したほうがよい。

```{r}
#| label: kwic
dat <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(section == "[1]上_先生と私") |>
  dplyr::select(label, token) |>
  dplyr::collect() |>
  dplyr::reframe(token = list(token), .by = label) |>
  tibble::deframe() |>
  quanteda::as.tokens() |>
  quanteda::tokens_select("[[:punct:]]", selection = "remove", valuetype = "regex", padding = FALSE) |>
  quanteda::tokens_select("^[\\p{Hiragana}]{1,2}$", selection = "remove", valuetype = "regex", padding = TRUE)

quanteda::kwic(dat, pattern = "^向[いくけこ]$", window = 5, valuetype = "regex")
```

なお、上の例ではひらがな1～2文字の語をpaddingしつつ除外したので、一部の助詞などは表示されていない（それぞれの窓のなかでトークンとして数えられてはいる）。

### コロケーション

たとえば、前後5個のwindow内のコロケーション（nodeを含めて11語の窓ということ）の合計については次のように確認できる。

```{r}
#| label: collocation-1
dat |>
  quanteda::fcm(context = "window", window = 5) |>
  tidytext::tidy() |>
  dplyr::rename(node = document, term = term) |>
  dplyr::filter(node == "向い") |>
  dplyr::slice_max(count, n = 10)
```

「左合計」や「右合計」については、たとえば次のようにして確認できる。paddingしなければ`tidyr::separate_wider_delim()`で展開して位置ごとに集計することもできそう。

```{r}
#| label: collocation-2
dat |>
  quanteda::kwic(pattern = "^向[いくけこ]$", window = 5, valuetype = "regex") |>
  dplyr::as_tibble() |>
  dplyr::select(docname, keyword, pre, post) |>
  tidyr::pivot_longer(
    c(pre, post),
    names_to = "window",
    values_to = "term",
    values_transform = ~ strsplit(., " ", fixed = TRUE)
  ) |>
  tidyr::unnest(term) |>
  dplyr::count(window, term, sort = TRUE)
```

## 関連語検索（A.5.6）

### 関連語のリスト

「確率差」や「確率比」については、いちおう計算はできた気がするが、あっているのかよくわからない。また、このやり方はそれなりの数の共起について計算をしなければならず、共起行列が大きくなると大変そう。

```{r}
#| label: calc-co-measures
dfm <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    section == "[1]上_先生と私",
    pos %in% c(
      "名詞", #"名詞B", "名詞C",
      "地名", "人名", "組織名", "固有名詞",
      "動詞", "未知語", "タグ"
    )
  ) |>
  dplyr::mutate(
    token = dplyr::if_else(is.na(original), token, original),
    token = paste(token, pos, sep = "/")
  ) |>
  dplyr::count(doc_id, token) |>
  dplyr::collect() |>
  tidytext::cast_dfm(doc_id, token, n) |>
  quanteda::dfm_weight(scheme = "boolean")

dat <- dfm |>
  quanteda::fcm() |>
  tidytext::tidy() |>
  dplyr::rename(target = document, co_occur = count) |>
  dplyr::reframe(
    term = term,
    target_occur = quanteda::colSums(dfm)[target],
    term_occur = quanteda::colSums(dfm)[term],
    co_occur = co_occur,
    .by = target
  ) |>
  dplyr::mutate(
    p_x = target_occur / quanteda::ndoc(dfm),
    p_y = term_occur / quanteda::ndoc(dfm),
    p_xy = (co_occur / quanteda::ndoc(dfm)) / p_x,
    differential = p_xy - p_y, # 確率差
    lift = p_xy / p_y, # 確率比（リフト）,
    jaccard = co_occur / (target_occur + term_occur - co_occur),
    dice = 2 * co_occur / (target_occur + term_occur)
  ) |>
  dplyr::select(target, term, differential, lift, jaccard, dice)

dat
```

### 共起ネットワーク

「先生/名詞」と関連の強そうな語の共起を図示した例。

「先生/名詞」と共起している語のうち、出現回数が上位20位以内である語が`target`である共起を抽出したうえで、それらのなかからJaccard係数が大きい順に75個だけ残している。「先生/名詞」という語そのものは図に含めていない。

```{r}
#| label: plot-co-measures
#| cache: true
dat |>
  dplyr::inner_join(
    dplyr::filter(dat, target == "先生/名詞") |> dplyr::select(term),
    by = dplyr::join_by(target == term)
  ) |>
  dplyr::filter(target %in% names(quanteda::topfeatures(dfm, 20))) |>
  dplyr::slice_max(jaccard, n = 75) |>
  tidygraph::as_tbl_graph(directed = FALSE) |>
  tidygraph::to_minimum_spanning_tree() |>
  purrr::pluck("mst") |>
  dplyr::mutate(
    community = factor(tidygraph::group_leading_eigen())
  ) |>
  ggraph::ggraph(layout = "fr") +
  ggraph::geom_edge_link(aes(width = sqrt(lift), alpha = jaccard)) +
  ggraph::geom_node_point(aes(colour = community), show.legend = FALSE) +
  ggraph::geom_node_text(aes(label = name, colour = community), repel = TRUE, show.legend = FALSE) +
  ggraph::theme_graph()
```

### アソシエーション分析🍳

英語だとこのメニューの名前は「Word Association」となっているので、ふつうにアソシエーション分析すればいいと思った。

arulesの`transactions`オブジェクトをつくるには、quantedaの`fcm`オブジェクトから変換すればよい（arulesをアタッチしている必要がある）。

```{r}
#| label: create-transactions
library(arules)
library(arulesViz)

dat <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    pos %in% c(
      "名詞", #"名詞B", "名詞C",
      "地名", "人名", "組織名", "固有名詞",
      "動詞", "未知語", "タグ"
    )
  ) |>
  dplyr::mutate(
    token = dplyr::if_else(is.na(original), token, original),
    token = paste(token, pos, sep = "/")
  ) |>
  dplyr::count(doc_id, token) |>
  dplyr::collect() |>
  tidytext::cast_dfm(doc_id, token, n) |>
  quanteda::dfm_weight(scheme = "boolean") |>
  quanteda::fcm() |>
  as("nMatrix") |>
  as("transactions")
```

`arules::apriori()`でアソシエーションルールを抽出する。

```{r}
#| label: apriori
#| cache: true
rules <-
  arules::apriori(
    dat,
    parameter = list(
      support = 0.075,
      confidence = 0.8,
      minlen = 2,
      maxlen = 2, # LHS+RHSの長さ。変えないほうがよい
      maxtime = 5
    ),
    control = list(verbose = FALSE)
  )
```

この形式のオブジェクトは`as(rules, "data.frame")`のようにしてデータフレームに変換できる。`tibble`にしたい場合には次のようにすればよい。

```{r}
#| label: glimpse-rules
as(rules, "data.frame") |>
  dplyr::mutate(across(where(is.numeric), ~ signif(., digits = 3))) |>
  tidyr::separate_wider_delim(rules, delim = " => ", names = c("lhs", "rhs")) |>
  dplyr::arrange(desc(lift))
```

### 散布図🍳

```{r}
#| label: plot-rules-scatter
plot(rules, engine = "html")
```

### バルーンプロット🍳

```{r}
#| label: plot-rules-grouped
plot(rules, method = "grouped", engine = "html")
```

### ネットワーク図🍳

```{r}
#| label: plot-rules-graph
plot(rules, method = "graph", engine = "html")
```

## 対応分析（A.5.7）

### コレスポンデンス分析

段落（`doc_id`）内の頻度で語彙を削ってから部（`section`）ごとに集計するために、ややめんどうなことをしている。

```{r}
#| label: create-dfm
dfm <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    pos %in% c(
      "名詞", "名詞B", "名詞C",
      "地名", "人名", "組織名", "固有名詞",
      "動詞", "未知語", "タグ"
    )
  ) |>
  dplyr::mutate(
    token = dplyr::if_else(is.na(original), token, original),
    token = paste(token, pos, sep = "/")
  ) |>
  dplyr::count(doc_id, token) |>
  dplyr::collect() |>
  tidytext::cast_dfm(doc_id, token, n) |>
  quanteda::dfm_trim(
    min_termfreq = 75,
    termfreq_type = "rank",
    min_docfreq = 30,
    docfreq_type = "count"
  )
```

こうして`doc_id`ごとに集計した`dfm`オブジェクトを一度`tidytext::tidy()`して3つ組のデータフレームに戻し、`section`のラベルを結合する。このデータフレームをもう一度`tidytext::cast_dfm()`で疎行列に変換して、`quanteda.textmodels::textmodel_ca()`を使って対応分析にかける。

```{r}
#| label: fit-ca
#| cache: true
ca_fit <- dfm |>
  tidytext::tidy() |>
  dplyr::left_join(
    dplyr::select(tbl, doc_id, section),
    by = dplyr::join_by(document == doc_id)
  ) |>
  tidytext::cast_dfm(section, term, count) |>
  quanteda.textmodels::textmodel_ca(nd = 2, sparse = TRUE)
```

この関数は疎行列に対して計算をおこなえるため、比較的大きな行列を渡しても大丈夫。

### バイプロット

caパッケージを読み込んでいると`plot()`でバイプロットを描ける。`factoextra::fviz_ca_biplot()`でも描けるが、見た目は`plot()`のとあまり変わらない。

```{r}
#| label: plot-ca-1
library(ca)
dat <- plot(ca_fit)
```

### バイプロット（バブルプロット）

ggplot2でバイプロットを描画するには、たとえば次のようにする。`ggrepel::geom_text_repel`でラベルを出す語彙の選択の仕方はもうすこし工夫したほうがよいかもしれない。

なお、このコードは[Correspondence Analysis visualization using ggplot | R-bloggers](https://www.r-bloggers.com/2019/08/correspondence-analysis-visualization-using-ggplot/)を参考にした。

```{r}
#| label: plot-ca-2
tf <- dfm |>
  tidytext::tidy() |>
  dplyr::left_join(
    dplyr::select(tbl, doc_id, section),
    by = dplyr::join_by(document == doc_id)
  ) |>
  dplyr::summarise(tf = sum(count), .by = term) |>
  dplyr::pull(tf, term)

# modified from https://www.r-bloggers.com/2019/08/correspondence-analysis-visualization-using-ggplot/
make_ca_plot_df <- function(ca.plot.obj, row.lab = "Rows", col.lab = "Columns") {
  tibble::tibble(
    Label = c(
      rownames(ca.plot.obj$rows),
      rownames(ca.plot.obj$cols)
    ),
    Dim1 = c(
      ca.plot.obj$rows[, 1],
      ca.plot.obj$cols[, 1]
    ),
    Dim2 = c(
      ca.plot.obj$rows[, 2],
      ca.plot.obj$cols[, 2]
    ),
    Variable = c(
      rep(row.lab, nrow(ca.plot.obj$rows)),
      rep(col.lab, nrow(ca.plot.obj$cols))
    )
  )
}
dat <- dat |>
  make_ca_plot_df(row.lab = "Construction", col.lab = "Medium") |>
  dplyr::mutate(
    Size = dplyr::if_else(Variable == "Construction", mean(tf), tf[Label])
  )
# 非ASCII文字のラベルに対してwarningを出さないようにする
suppressWarnings({
  ca_sum <- summary(ca_fit)
  dim_var_percs <- ca_sum$scree[, "values2"]
})

dat |>
  ggplot(aes(x = Dim1, y = Dim2, col = Variable, label = Label)) +
  geom_vline(xintercept = 0, lty = "dashed", alpha = .5) +
  geom_hline(yintercept = 0, lty = "dashed", alpha = .5) +
  geom_jitter(aes(size = Size), alpha = .3, show.legend = FALSE) +
  ggrepel::geom_label_repel(
    data = \(x) dplyr::filter(x, Variable == "Construction"),
    show.legend = FALSE
  ) +
  ggrepel::geom_text_repel(
    data = \(x) dplyr::filter(x, Variable == "Medium", sqrt(Dim1^2 + Dim2^2) > 0.25),
    show.legend = FALSE
  ) +
  scale_x_continuous(
    limits = range(dat$Dim1) +
      c(diff(range(dat$Dim1)) * -0.2, diff(range(dat$Dim1)) * 0.2)) +
  scale_y_continuous(
    limits = range(dat$Dim2) +
      c(diff(range(dat$Dim2)) * -0.2, diff(range(dat$Dim2)) * 0.2)) +
  scale_size_area(max_size = 16) +
  labs(
    x = paste0("Dimension 1 (", signif(dim_var_percs[1], 3), "%)"),
    y = paste0("Dimension 2 (", signif(dim_var_percs[2], 3), "%)")
  ) +
  theme_classic()
```

## 多次元尺度構成法（A.5.8）

### MDS・バブルプロット

`MASS::isoMDS`より`MASS::sammon`のほうがたぶん見やすい。

```{r}
#| label: mds-2d
#| cache: true
simil <- dfm |>
  quanteda::dfm_weight(scheme = "boolean") |>
  proxyC::simil(margin = 2, method = "jaccard")

dat <- MASS::sammon(1 - simil, k = 2) |>
  purrr::pluck("points")
```

```{r}
#| label: plot-mds-2d
dat <- dat |>
  as.data.frame() |>
  tibble::rownames_to_column("label") |>
  dplyr::rename(Dim1 = V1, Dim2 = V2) |>
  dplyr::mutate(
    size = tf[label],
    clust = (hclust(
      proxyC::dist(dat, method = "euclidean") |> as.dist(),
      method = "ward.D2"
    ) |> cutree(k = 6))[label]
  )

dat |>
  ggplot(aes(x = Dim1, y = Dim2, label = label, col = factor(clust))) +
  geom_point(aes(size = size), alpha = .3, show.legend = FALSE) +
  ggrepel::geom_text_repel(show.legend = FALSE) +
  scale_size_area(max_size = 16) +
  theme_classic()
```

### MDS・3次元プロット

rglではなくplotlyで試してみたが、とくに見やすいということはなかったかもしれない。

```{r}
#| label: mds-3d
#| cache: true
dat <- MASS::sammon(1 - simil, k = 3) |>
  purrr::pluck("points") |>
  as.data.frame() |>
  tibble::rownames_to_column("label") |>
  dplyr::rename(Dim1 = V1, Dim2 = V2, Dim3 = V3) |>
  dplyr::mutate(term_freq = tf[label])
```

```{r}
#| label: plot-mds-3d
dat |>
  plotly::plot_ly(x = ~Dim1, y = ~Dim2, z = ~Dim3, text = ~label, color = ~term_freq) |>
  plotly::add_markers(opacity = .5)
```

## 階層的クラスター分析（A.5.9）

### 非類似度のヒートマップ🍳

Jaccard係数を指定して非類似度のヒートマップを描くと、そもそもパターンが何も見えなかった。

```{r}
#| label: plot-heatmap
dat <- dfm |>
  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = "rank") |>
  quanteda::dfm_weight(scheme = "boolean") |>
  proxyC::simil(margin = 2, method = "dice") |>
  rlang::as_function(~ 1 - .)()

factoextra::fviz_dist(as.dist(dat))
```

### 階層的クラスタリング

```{r}
#| label: hclust
clusters <-
  as.dist(dat) |>
  hclust(method = "ward.D2")
```

### シルエット分析🍳

```{r}
#| label: plot-hclust-silhoutte-1
factoextra::fviz_nbclust(
  as.matrix(dat),
  FUNcluster = factoextra::hcut,
  k.max = ceiling(sqrt(nrow(dat)))
)
```

```{r}
#| label: plot-hclust-silhoutte-2
cluster::silhouette(cutree(clusters, k = 4), dist = dat) |>
  factoextra::fviz_silhouette(print.summary = FALSE) +
  theme_classic()
```

### デンドログラム

デンドログラムについては、似たような表現を手軽に実現できる方法が見つけられない。ラベルの位置が左右反転しているが、`factoextra::fviz_dend(horiz = TRUE)`とするのが簡単かもしれない。

```{r}
#| label: plot-hclust-factoextra
factoextra::fviz_dend(clusters, h = 1.0, horiz = TRUE, labels_track_height = 0.3)
```

### デンドログラムと棒グラフ

KH Coderのソースコードを見た感じ、デンドログラムと一緒に語の出現回数を描いている表現は、やや独特なことをしている。むしろ語の出現回数のほうが主な情報になってよいなら、ふつうの棒グラフの横に`ggh4x::scale_y_dendrogram()`でデンドログラムを描くことができる。

```{r}
#| label: plot-hclust-ggplot2
dfm |>
  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = "rank") |>
  quanteda::colSums() |>
  tibble::enframe() |>
  dplyr::mutate(
    clust = (clusters |> cutree(h = 1.0))[name]
  ) |>
  ggplot(aes(x = value, y = name, fill = factor(clust))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_x_sqrt() +
  ggh4x::scale_y_dendrogram(hclust = clusters) +
  labs(x = "出現回数", y = element_blank()) +
  theme_bw()
```

## 共起ネットワーク

## 自己組織化マップ（A.5.10）

### 自己組織化マップ（SOM）

SOMの実装としては、KH Coderは[som](https://cran.r-project.org/package=som)を使っているようだが、[kohonen](https://cran.r-project.org/package=kohonen)を使ったほうがよい。行列が非常に大きい場合には`kohonen::som(mode = "online")`としてもよいのかもしれないが、一般にバッチ型のほうが収束が早く、数十ステップ程度回せば十分である。

与える単語文書行列は、ここでは`tidytext::bind_tf_idf()`を使ってTF-IDFで重みづけし、上位100語ほど抽出する。

```{r}
#| label: fit-som
#| cache: true
dat <-
  dplyr::tbl(con, "tokens") |>
  dplyr::filter(
    pos %in% c(
      "名詞", #"名詞B", "名詞C",
      "地名", "人名", "組織名", "固有名詞",
      "動詞", "未知語", "タグ"
    )
  ) |>
  dplyr::mutate(
    token = dplyr::if_else(is.na(original), token, original),
    token = paste(token, pos, sep = "/")
  ) |>
  dplyr::count(doc_id, token) |>
  dplyr::collect() |>
  tidytext::bind_tf_idf(token, doc_id, n) |>
  tidytext::cast_dfm(doc_id, token, tf_idf) |>
  quanteda::dfm_trim(
    min_termfreq = 100,
    termfreq_type = "rank"
  ) |>
  as.matrix() |>
  scale() |>
  t()

som_fit <-
  kohonen::som(
    dat,
    grid = kohonen::somgrid(16, 12, "hexagonal"),
    rlen = 50, # 学習回数
    alpha = c(0.05, 0.01),
    radius = 6,
    dist.fcts = "sumofsquares",
    mode = "pbatch",
    init = aweSOM::somInit(dat, 16, 12)
  )
```

```{r}
#| label: quality-som
aweSOM::somQuality(som_fit, dat)
```

### U-Matrix

U-matrixは「各ノードの参照ベクトルが近傍ノードと異なる度合いで色づけする方法」（[自己組織化マップ入門](https://www.brain.kyutech.ac.jp/~furukawa/data/SOMtext.pdf)）。暖色の箇所はデータ密度が低い「山間部」で、寒色の箇所はデータ密度が高い「平野部」みたいなイメージ、写像の勾配が急峻になっている箇所を境にしてクラスタが分かれていると判断するみたいな見方をする。

```{r}
#| label: plot-umatrix
aweSOM::aweSOMsmoothdist(som_fit)
```

```{r}
#| label: plot-umatrix-interactive
aweSOM::aweSOMplot(
  som_fit,
  data = dat,
  type = "UMatrix"
)
```

### ヒットマップ🍳

`clusters::pam()`でクラスタリングして色を付ける。「山間部」に囲まれた一部の「盆地」がクラスタになって、後はその他の部分みたいな感じに分かれるようだが、解釈するのに便利な感じで分かれてはくれなかった。

```{r}
#| label: cluster-som
clusters <- som_fit |>
  purrr::pluck("codes", 1) |> # 参照ベクトル（codebook vectors）は`codes`にリストとして格納されている
  cluster::pam(k = 8)
```

ヒットマップ（hitmap, proportion map）は以下のような可視化の方法。ノードの中の六角形は各ノードが保持する参照ベクトルの数（比率）を表している。ノードの背景色が上のコードで得たクラスタに対応する。

```{r}
#| label: plot-hitmap
aweSOM::aweSOMplot(
  som_fit,
  data = dat,
  type = "Hitmap",
  superclass = clusters[["clustering"]]
)
```
---

```{r}
#| label: cleanup
duckdb::dbDisconnect(con)
duckdb::duckdb_shutdown(drv)

sessioninfo::session_info(info = "packages")
```
